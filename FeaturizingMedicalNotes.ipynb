{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /Users/ahara/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/ahara/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 375
    },
    "id": "63VhfvhAIcUO",
    "outputId": "31baa97f-dc4b-40df-8472-c88b7ab54ea7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded word vectors successfully!\n"
     ]
    }
   ],
   "source": [
    "# File sent in email\n",
    "word2vec = gensim.models.KeyedVectors.load_word2vec_format('BioWordVec_PubMed_MIMICIII_d200.vec.bin', binary=True)\n",
    "# https://github.com/ncbi-nlp/BioSentVec 1st download (13GB)\n",
    "# word2vecPubMed = gensim.models.KeyedVectors.load_word2vec_format(\"FILE-NAME.bin.gz\", binary=True)\n",
    "# https://github.com/ncbi-nlp/BioSentVec 2nd download (21GB)\n",
    "# sent2vecPubMed = gensim.models.KeyedVectors.load_word2vec_format(\"FILE-NAME.bin.gz\", binary=True)\n",
    "\n",
    "print(\"Loaded word vectors successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def doc_to_vec(sentence, word2vec):\n",
    "    word_vecs = [word2vec.get_vector(w) for w in sentence.split() if w in word2vec.vocab]\n",
    "    if len(word_vecs) != 0:\n",
    "        stack = np.asarray(word_vecs).mean(0)\n",
    "        if stack.all() != None:\n",
    "            if isinstance(stack, (np.ndarray, np.generic)):\n",
    "                return np.stack(stack)\n",
    "            else:\n",
    "                print(\"error\")\n",
    "\n",
    "def featurize(df, file):\n",
    "    features = dict()\n",
    "    for index, row in df.iterrows():\n",
    "        # Change column header according to the csv file\n",
    "        noteText = row['noteText']\n",
    "        if isinstance(noteText, str):\n",
    "            tokens = tokenizer.tokenize(noteText.lower())\n",
    "            tokens = [word for word in tokens if word not in stopwords.words('english')]\n",
    "            my_string = ' '.join(tokens)\n",
    "            googleVector = doc_to_vec(my_string, word2vec)\n",
    "            features[row['mrn']] = googleVector\n",
    "    featuresDF = pd.DataFrame(pd.Series(features).reset_index())\n",
    "    featuresDF.to_csv(file, mode='a', header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Read in medical notes -- Change file name accordingly\n",
    "colNames = ['mrn', 'noteType', 'noteText']\n",
    "for chunk in pd.read_csv('merged_trainNotesAbridged.csv', header=None, error_bad_lines=False, chunksize=1000, names=colNames, encoding='latin-1'):\n",
    "    featurize(chunk, \"notesVectorized_pubmed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "drjsXI3hLmZ1"
   },
   "outputs": [],
   "source": [
    "colNames = ['mrn', 'noteType', 'noteText']\n",
    "for chunk in pd.read_csv('merged_devNotesAbridged.csv', header=None, error_bad_lines=False, chunksize=1000, skiprows=[0], names=colNames, encoding='latin-1'):\n",
    "    featurize(chunk, \"dev_notesVectorized_pubmed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6412\n",
      "2139\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "cols1=['index', 'mrn', 'vector']\n",
    "cols2=['mrn', 'label']\n",
    "\n",
    "notesW2V = pd.read_csv('notesVectorized_pubmed.csv', dtype=object, header=None, names=cols1)\n",
    "labelsW2V = pd.read_csv('train_data.csv', header=None, names=cols2)\n",
    "\n",
    "notesW2V['mrn'].replace('', np.nan, inplace=True)\n",
    "notesW2V.dropna(subset=['mrn'], inplace=True)\n",
    "\n",
    "notesW2V['mrn'] = notesW2V['mrn'].astype(int)\n",
    "labelsW2V['mrn'] = labelsW2V['mrn'].astype(int)\n",
    "\n",
    "notesAndLabelsW2V = pd.merge(notesW2V, labelsW2V, how='left', on=['mrn'])\n",
    "\n",
    "kerasXW2V = []\n",
    "kerasYW2V = []\n",
    "for index, row in notesAndLabelsW2V.iterrows():\n",
    "    vectorRow = []\n",
    "    row2 = \"\"\n",
    "    row2 = row['vector'].replace(\"[\", '').replace(\"]\", '')\n",
    "    row2 = re.split('\\\\s+', row2)\n",
    "    for i in row2:\n",
    "        if i != \"\":\n",
    "            vectorRow.append(float(i))\n",
    "    kerasXW2V.append(vectorRow)\n",
    "    kerasYW2V.append(row['label'])\n",
    "\n",
    "print(len(kerasYW2V))\n",
    "\n",
    "cols1=['index', 'mrn', 'vector']\n",
    "cols2=['mrn', 'label']\n",
    "\n",
    "notes2W2V = pd.read_csv('dev_notesVectorized_pubmed.csv', dtype=object, header=None, names=cols1)\n",
    "labels2W2V = pd.read_csv('validate_data.csv', header=None, names=cols2)\n",
    "\n",
    "notes2W2V['mrn'].replace('', np.nan, inplace=True)\n",
    "notes2W2V.dropna(subset=['mrn'], inplace=True)\n",
    "\n",
    "notes2W2V['mrn'] = notes2W2V['mrn'].astype(int)\n",
    "labels2W2V['mrn'] = labels2W2V['mrn'].astype(int)\n",
    "\n",
    "notesAndLabels2W2V = pd.merge(notes2W2V, labels2W2V, how='left', on=['mrn'])\n",
    "\n",
    "kerasXDevW2V = []\n",
    "kerasYDevW2V =[]\n",
    "for index, row in notesAndLabels2W2V.iterrows():\n",
    "    vectorRow = []\n",
    "    row2 = \"\"\n",
    "    row2 = row['vector'].replace(\"[\", '').replace(\"]\", '')\n",
    "    row2 = re.split('\\\\s+', row2)\n",
    "    for i in row2:\n",
    "        if i != \"\":\n",
    "            vectorRow.append(float(i))\n",
    "    kerasXDevW2V.append(vectorRow)\n",
    "    kerasYDevW2V.append(row['label'])\n",
    "print(len(kerasYDevW2V))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import adjusted_rand_score\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "notes= pd.read_csv('merged_trainNotesAbridged.csv', header=None, error_bad_lines=False, encoding='latin-1')\n",
    "notes2= pd.read_csv('merged_devNotesAbridged.csv', header=None, error_bad_lines=False,encoding='latin-1')\n",
    "notes3= pd.read_csv('merged-testNotesAbridged.csv', header=None, error_bad_lines=False, encoding='latin-1')\n",
    "\n",
    "for index, row in notes.iterrows():\n",
    "    if not isInt(row[1]):\n",
    "        notes = notes.drop([index])\n",
    "\n",
    "for index, row in notes2.iterrows():\n",
    "    if not isInt(row[1]):\n",
    "        notes2 = notes2.drop([index])\n",
    "        \n",
    "for index, row in notes3.iterrows():\n",
    "    if not isInt(row[1]):\n",
    "        notes3 = notes3.drop([index])\n",
    "\n",
    "notes = notes.rename(columns={0: \"index\", 1: \"mrn\", 2: \"noteType\", 3: \"noteText\"})\n",
    "notes2 = notes2.rename(columns={0: \"index\", 1: \"mrn\", 2: \"noteType\", 3: \"noteText\"})\n",
    "notes3 = notes3.rename(columns={0: \"index\", 1: \"mrn\", 2: \"noteType\", 3: \"noteText\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>mrn</th>\n",
       "      <th>noteType</th>\n",
       "      <th>noteText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>000645514</td>\n",
       "      <td>Nursing Adult Admission History; Nursing Adult...</td>\n",
       "      <td>Preferred Language:    \\r\\n\\r\\n Preferred Lang...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>000738305</td>\n",
       "      <td>ED Adult Pre-Assessment Note; ED Adult Pre-Ass...</td>\n",
       "      <td>Triage Information:   \\r\\n\\r\\n  Triage Informa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.0</td>\n",
       "      <td>000816573</td>\n",
       "      <td>ED Patient Discharge Form; ED Patient Discharg...</td>\n",
       "      <td>Interpreter Services:    \\r\\n\\r\\n Services Req...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.0</td>\n",
       "      <td>000855972</td>\n",
       "      <td>ED Adult Pre-Assessment Note; ED Adult Pre-Ass...</td>\n",
       "      <td>Triage Information:   \\r\\n\\r\\n  Triage Informa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4.0</td>\n",
       "      <td>000921689</td>\n",
       "      <td>Nursing Adult Admission History; Patient Disch...</td>\n",
       "      <td>Preferred Language:    \\r\\n\\r\\n  Preferred Lan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6408</th>\n",
       "      <td>6407.0</td>\n",
       "      <td>086454304</td>\n",
       "      <td>ED Adult Pre-Assessment Note; ED Adult Pre-Ass...</td>\n",
       "      <td>Pollen        Unknown           \\r\\n\\r\\n  I ha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6409</th>\n",
       "      <td>6408.0</td>\n",
       "      <td>086628976</td>\n",
       "      <td>ED Adult Pre-Assessment Note; ED Adult Pre-Ass...</td>\n",
       "      <td>Patient preferred language:    \\r\\n\\r\\n Prefer...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6410</th>\n",
       "      <td>6409.0</td>\n",
       "      <td>088307440</td>\n",
       "      <td>ED Patient Discharge Form; ED Patient Discharg...</td>\n",
       "      <td>Type        Entered By        Otitis media lef...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6411</th>\n",
       "      <td>6410.0</td>\n",
       "      <td>089681932</td>\n",
       "      <td>ED Unified Attending Note; ED Unified Attendin...</td>\n",
       "      <td>ablation (2010) anxiety(1)         Past Surger...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6412</th>\n",
       "      <td>6411.0</td>\n",
       "      <td>089833204</td>\n",
       "      <td>ED Adult Pre-Assessment Note; ED Adult Pre-Ass...</td>\n",
       "      <td>Patient preferred language:    \\r\\n\\r\\n Prefer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6412 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       index              mrn  \\\n",
       "1        0.0  000645514         \n",
       "2        1.0  000738305         \n",
       "3        2.0  000816573         \n",
       "4        3.0  000855972         \n",
       "5        4.0  000921689         \n",
       "...      ...              ...   \n",
       "6408  6407.0  086454304         \n",
       "6409  6408.0  086628976         \n",
       "6410  6409.0  088307440         \n",
       "6411  6410.0  089681932         \n",
       "6412  6411.0  089833204         \n",
       "\n",
       "                                               noteType  \\\n",
       "1     Nursing Adult Admission History; Nursing Adult...   \n",
       "2     ED Adult Pre-Assessment Note; ED Adult Pre-Ass...   \n",
       "3     ED Patient Discharge Form; ED Patient Discharg...   \n",
       "4     ED Adult Pre-Assessment Note; ED Adult Pre-Ass...   \n",
       "5     Nursing Adult Admission History; Patient Disch...   \n",
       "...                                                 ...   \n",
       "6408  ED Adult Pre-Assessment Note; ED Adult Pre-Ass...   \n",
       "6409  ED Adult Pre-Assessment Note; ED Adult Pre-Ass...   \n",
       "6410  ED Patient Discharge Form; ED Patient Discharg...   \n",
       "6411  ED Unified Attending Note; ED Unified Attendin...   \n",
       "6412  ED Adult Pre-Assessment Note; ED Adult Pre-Ass...   \n",
       "\n",
       "                                               noteText  \n",
       "1     Preferred Language:    \\r\\n\\r\\n Preferred Lang...  \n",
       "2     Triage Information:   \\r\\n\\r\\n  Triage Informa...  \n",
       "3     Interpreter Services:    \\r\\n\\r\\n Services Req...  \n",
       "4     Triage Information:   \\r\\n\\r\\n  Triage Informa...  \n",
       "5     Preferred Language:    \\r\\n\\r\\n  Preferred Lan...  \n",
       "...                                                 ...  \n",
       "6408  Pollen        Unknown           \\r\\n\\r\\n  I ha...  \n",
       "6409  Patient preferred language:    \\r\\n\\r\\n Prefer...  \n",
       "6410  Type        Entered By        Otitis media lef...  \n",
       "6411  ablation (2010) anxiety(1)         Past Surger...  \n",
       "6412  Patient preferred language:    \\r\\n\\r\\n Prefer...  \n",
       "\n",
       "[6412 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6412\n",
      "2139\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "cols1=['index', 'mrn', 'noteType', 'noteText']\n",
    "cols2=['mrn', 'label']\n",
    "\n",
    "labels = pd.read_csv('train_data.csv', header=None, names=cols2)\n",
    "\n",
    "notes['mrn'].replace('', np.nan, inplace=True)\n",
    "notes.dropna(subset=['mrn'], inplace=True)\n",
    "\n",
    "notes['mrn'] = notes['mrn'].astype(int)\n",
    "labels['mrn'] = labels['mrn'].astype(int)\n",
    "\n",
    "notesAndLabels = pd.merge(notes, labels, how='left', on=['mrn'])\n",
    "\n",
    "kerasX = np.asarray(notesAndLabels['noteText'])\n",
    "kerasY = np.asarray(notesAndLabels['label'])\n",
    "print(len(kerasY))\n",
    "\n",
    "cols1=['index', 'mrn', 'vector']\n",
    "cols2=['mrn', 'label']\n",
    "\n",
    "labels2 = pd.read_csv('validate_data.csv', header=None, names=cols2)\n",
    "\n",
    "notes2['mrn'].replace('', np.nan, inplace=True)\n",
    "notes2.dropna(subset=['mrn'], inplace=True)\n",
    "\n",
    "notes2['mrn'] = notes2['mrn'].astype(int)\n",
    "labels2['mrn'] = labels2['mrn'].astype(int)\n",
    "\n",
    "notesAndLabels2 = pd.merge(notes2, labels2, how='left', on=['mrn'])\n",
    "\n",
    "kerasXDev = np.asarray(notesAndLabels2['noteText'])\n",
    "kerasYDev = np.asarray(notesAndLabels2['label'])\n",
    "print(len(kerasYDev))\n",
    "\n",
    "labels3 = pd.read_csv('test_data.csv', header=None, names=cols2)\n",
    "\n",
    "notes3['mrn'].replace('', np.nan, inplace=True)\n",
    "notes3.dropna(subset=['mrn'], inplace=True)\n",
    "\n",
    "notes3['mrn'] = notes3['mrn'].astype(int)\n",
    "labels3['mrn'] = labels3['mrn'].astype(int)\n",
    "\n",
    "notesAndLabels3 = pd.merge(notes3, labels3, how='left', on=['mrn'])\n",
    "\n",
    "kerasXTest = np.asarray(notesAndLabels3['noteText'])\n",
    "kerasYTest = np.asarray(notesAndLabels3['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for i in kerasYDev:\n",
    "    if i == 1:\n",
    "        count += 1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(kerasX)\n",
    "Xpred = vectorizer.transform(kerasXDev)\n",
    "Xtest = vectorizer.transform(kerasXTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred,y):\n",
    "    predictions = []\n",
    "    for i in y_pred:\n",
    "        if i > .5:\n",
    "            predictions.append(1)\n",
    "        else:\n",
    "            predictions.append(0)\n",
    "    accuracy = accuracy_score(y, predictions)\n",
    "    precision = precision_score(y, predictions)\n",
    "    recall = recall_score(y, predictions)\n",
    "\n",
    "    print(accuracy)\n",
    "    print(precision)\n",
    "    print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9453015427769986\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "test = [0] * len(kerasYDevW2VG)\n",
    "accuracy(test, kerasYDevW2VG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(n):\n",
    "    y_pred=[]\n",
    "    knn = KNeighborsClassifier(n_neighbors=n, metric='euclidean')\n",
    "    knn.fit(X, kerasY)\n",
    "    \n",
    "    y_pred = knn.predict(Xpred)\n",
    "    accuracy(y_pred, kerasYDevW2VG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9130434782608695\n",
      "0.38461538461538464\n",
      "0.2830188679245283\n",
      "0.9406264609630669\n",
      "0.08547008547008547\n",
      "0.3333333333333333\n",
      "0.9345488546049556\n",
      "0.3076923076923077\n",
      "0.37894736842105264\n",
      "0.9467040673211781\n",
      "0.11965811965811966\n",
      "0.56\n",
      "0.9429640018700327\n",
      "0.21367521367521367\n",
      "0.45454545454545453\n",
      "0.946236559139785\n",
      "0.08547008547008547\n",
      "0.5555555555555556\n",
      "0.949041608228144\n",
      "0.20512820512820512\n",
      "0.6\n",
      "0.9467040673211781\n",
      "0.08547008547008547\n",
      "0.5882352941176471\n",
      "0.9467040673211781\n",
      "0.1452991452991453\n",
      "0.5483870967741935\n",
      "0.9453015427769986\n",
      "0.06837606837606838\n",
      "0.5\n",
      "0.9443665264142123\n",
      "0.1282051282051282\n",
      "0.46875\n",
      "0.9448340345956054\n",
      "0.07692307692307693\n",
      "0.47368421052631576\n",
      "0.9443665264142123\n",
      "0.1111111111111111\n",
      "0.4642857142857143\n",
      "0.9457690509583918\n",
      "0.06837606837606838\n",
      "0.5333333333333333\n",
      "0.9443665264142123\n",
      "0.10256410256410256\n",
      "0.46153846153846156\n",
      "0.9457690509583918\n",
      "0.06837606837606838\n",
      "0.5333333333333333\n",
      "0.9457690509583918\n",
      "0.09401709401709402\n",
      "0.5238095238095238\n",
      "0.9467040673211781\n",
      "0.05128205128205128\n",
      "0.6666666666666666\n",
      "0.946236559139785\n",
      "0.08547008547008547\n",
      "0.5555555555555556\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,20):\n",
    "    knn(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted'].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-60-382d66944ab4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mclusters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-60-382d66944ab4>\u001b[0m in \u001b[0;36mclusters\u001b[0;34m(n)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mkmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKMeans\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_clusters\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkerasXW2VG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkmeans\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkerasXDevW2VG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkerasYDevW2VG\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"-----\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-7b68fec12ee0>\u001b[0m in \u001b[0;36maccuracy\u001b[0;34m(y, y_pred)\u001b[0m\n\u001b[1;32m      7\u001b[0m             \u001b[0mpredictions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_score\u001b[0;34m(y_true, y_pred, labels, pos_label, average, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1615\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1616\u001b[0m     \"\"\"\n\u001b[0;32m-> 1617\u001b[0;31m     p, _, _, _ = precision_recall_fscore_support(y_true, y_pred,\n\u001b[0m\u001b[1;32m   1618\u001b[0m                                                  \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m                                                  \u001b[0mpos_label\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m                           FutureWarning)\n\u001b[1;32m     71\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight, zero_division)\u001b[0m\n\u001b[1;32m   1431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"beta should be >=0 in the F-beta score\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m     labels = _check_set_wise_labels(y_true, y_pred, average, labels,\n\u001b[0m\u001b[1;32m   1434\u001b[0m                                     pos_label)\n\u001b[1;32m   1435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py\u001b[0m in \u001b[0;36m_check_set_wise_labels\u001b[0;34m(y_true, y_pred, average, labels, pos_label)\u001b[0m\n\u001b[1;32m   1261\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'multiclass'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m                 \u001b[0maverage_options\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'samples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1263\u001b[0;31m             raise ValueError(\"Target is %s but average='binary'. Please \"\n\u001b[0m\u001b[1;32m   1264\u001b[0m                              \u001b[0;34m\"choose another average setting, one of %r.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m                              % (y_type, average_options))\n",
      "\u001b[0;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting, one of [None, 'micro', 'macro', 'weighted']."
     ]
    }
   ],
   "source": [
    "def clusters(n):\n",
    "    kmeans = KMeans(n_clusters=n, random_state=0).fit(kerasXW2VG)\n",
    "    y_pred = kmeans.predict(kerasXDevW2VG)\n",
    "    accuracy(y_pred, kerasYDevW2VG)\n",
    "    print(\"-----\")\n",
    "\n",
    "for i in range(10,20):\n",
    "    clusters(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9476390836839644\n",
      "0.05982905982905983\n",
      "0.7777777777777778\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "clf = RandomForestClassifier(max_depth=1000000, random_state=0)\n",
    "clf.fit(kerasXW2VG, kerasYW2VG)\n",
    "y_pred = clf.predict(kerasXDevW2VG)\n",
    "accuracy(y_pred, kerasYDevW2VG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.31926461\n",
      "Iteration 2, loss = 0.20723282\n",
      "Iteration 3, loss = 0.18370183\n",
      "Iteration 4, loss = 0.16440194\n",
      "Iteration 5, loss = 0.14835305\n",
      "Iteration 6, loss = 0.13646676\n",
      "Iteration 7, loss = 0.12660016\n",
      "Iteration 8, loss = 0.11746249\n",
      "Iteration 9, loss = 0.10918876\n",
      "Iteration 10, loss = 0.10162657\n",
      "Iteration 11, loss = 0.09511954\n",
      "Iteration 12, loss = 0.08859449\n",
      "Iteration 13, loss = 0.08433768\n",
      "Iteration 14, loss = 0.07854424\n",
      "Iteration 15, loss = 0.07398583\n",
      "Iteration 16, loss = 0.06979172\n",
      "Iteration 17, loss = 0.06626446\n",
      "Iteration 18, loss = 0.06348556\n",
      "Iteration 19, loss = 0.06083681\n",
      "Iteration 20, loss = 0.05883967\n",
      "Iteration 21, loss = 0.05675556\n",
      "Iteration 22, loss = 0.05511104\n",
      "Iteration 23, loss = 0.05331348\n",
      "Iteration 24, loss = 0.05173317\n",
      "Iteration 25, loss = 0.05053320\n",
      "Iteration 26, loss = 0.04919189\n",
      "Iteration 27, loss = 0.04779181\n",
      "Iteration 28, loss = 0.04657928\n",
      "Iteration 29, loss = 0.04541263\n",
      "Iteration 30, loss = 0.04442435\n",
      "Iteration 31, loss = 0.04340680\n",
      "Iteration 32, loss = 0.04253042\n",
      "Iteration 33, loss = 0.04158348\n",
      "Iteration 34, loss = 0.04083042\n",
      "Iteration 35, loss = 0.03986557\n",
      "Iteration 36, loss = 0.03908142\n",
      "Iteration 37, loss = 0.03831176\n",
      "Iteration 38, loss = 0.03753591\n",
      "Iteration 39, loss = 0.03676729\n",
      "Iteration 40, loss = 0.03602775\n",
      "Iteration 41, loss = 0.03550092\n",
      "Iteration 42, loss = 0.03471422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'numpy.float64' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-48-fabc64a3310e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMLPClassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkerasY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXpred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkerasYDev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.float64' object is not callable"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "clf = MLPClassifier(random_state=None, max_iter=1000, verbose=True).fit(X, kerasY)\n",
    "y_pred = clf.predict(Xpred)\n",
    "\n",
    "print(clf.score(Xpred, kerasYDev, sample_weight=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y_pred' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-836da0e8fd03>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkerasYDev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprecision_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkerasYDev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrecall_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkerasYDev\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'y_pred' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(kerasYDev, y_pred)\n",
    "precision = precision_score(kerasYDev, y_pred)\n",
    "recall = recall_score(kerasYDev, y_pred)\n",
    "\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall) # recall is accuracy of homeless notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.29797395\n",
      "Iteration 2, loss = 0.23467652\n",
      "Iteration 3, loss = 0.23156494\n",
      "Iteration 4, loss = 0.22906956\n",
      "Iteration 5, loss = 0.22729047\n",
      "Iteration 6, loss = 0.22515532\n",
      "Iteration 7, loss = 0.22336600\n",
      "Iteration 8, loss = 0.22101825\n",
      "Iteration 9, loss = 0.21827558\n",
      "Iteration 10, loss = 0.21606945\n",
      "Iteration 11, loss = 0.21337450\n",
      "Iteration 12, loss = 0.21305489\n",
      "Iteration 13, loss = 0.20939887\n",
      "Iteration 14, loss = 0.20746236\n",
      "Iteration 15, loss = 0.20592017\n",
      "Iteration 16, loss = 0.20543420\n",
      "Iteration 17, loss = 0.20468159\n",
      "Iteration 18, loss = 0.20161754\n",
      "Iteration 19, loss = 0.20200271\n",
      "Iteration 20, loss = 0.19921743\n",
      "Iteration 21, loss = 0.19804313\n",
      "Iteration 22, loss = 0.19658539\n",
      "Iteration 23, loss = 0.19585630\n",
      "Iteration 24, loss = 0.19539838\n",
      "Iteration 25, loss = 0.19392625\n",
      "Iteration 26, loss = 0.19203935\n",
      "Iteration 27, loss = 0.19177384\n",
      "Iteration 28, loss = 0.19072766\n",
      "Iteration 29, loss = 0.18949796\n",
      "Iteration 30, loss = 0.18848195\n",
      "Iteration 31, loss = 0.18664175\n",
      "Iteration 32, loss = 0.18706171\n",
      "Iteration 33, loss = 0.18586628\n",
      "Iteration 34, loss = 0.18568549\n",
      "Iteration 35, loss = 0.18958107\n",
      "Iteration 36, loss = 0.18515656\n",
      "Iteration 37, loss = 0.18381557\n",
      "Iteration 38, loss = 0.18375314\n",
      "Iteration 39, loss = 0.18306139\n",
      "Iteration 40, loss = 0.18109163\n",
      "Iteration 41, loss = 0.17975851\n",
      "Iteration 42, loss = 0.18141991\n",
      "Iteration 43, loss = 0.17850101\n",
      "Iteration 44, loss = 0.17839472\n",
      "Iteration 45, loss = 0.17914249\n",
      "Iteration 46, loss = 0.17836885\n",
      "Iteration 47, loss = 0.17682071\n",
      "Iteration 48, loss = 0.17651912\n",
      "Iteration 49, loss = 0.17680111\n",
      "Iteration 50, loss = 0.17576609\n",
      "Iteration 51, loss = 0.17664050\n",
      "Iteration 52, loss = 0.17709338\n",
      "Iteration 53, loss = 0.17593177\n",
      "Iteration 54, loss = 0.17672260\n",
      "Iteration 55, loss = 0.17506871\n",
      "Iteration 56, loss = 0.17443007\n",
      "Iteration 57, loss = 0.17623613\n",
      "Iteration 58, loss = 0.17469641\n",
      "Iteration 59, loss = 0.17478513\n",
      "Iteration 60, loss = 0.17373053\n",
      "Iteration 61, loss = 0.17473499\n",
      "Iteration 62, loss = 0.17410510\n",
      "Iteration 63, loss = 0.17569842\n",
      "Iteration 64, loss = 0.17373956\n",
      "Iteration 65, loss = 0.17167741\n",
      "Iteration 66, loss = 0.17306406\n",
      "Iteration 67, loss = 0.17647202\n",
      "Iteration 68, loss = 0.17101654\n",
      "Iteration 69, loss = 0.17095754\n",
      "Iteration 70, loss = 0.17139873\n",
      "Iteration 71, loss = 0.17061493\n",
      "Iteration 72, loss = 0.17036847\n",
      "Iteration 73, loss = 0.17290430\n",
      "Iteration 74, loss = 0.17230508\n",
      "Iteration 75, loss = 0.17290492\n",
      "Iteration 76, loss = 0.17220920\n",
      "Iteration 77, loss = 0.16961272\n",
      "Iteration 78, loss = 0.16990091\n",
      "Iteration 79, loss = 0.16913381\n",
      "Iteration 80, loss = 0.16925527\n",
      "Iteration 81, loss = 0.16976484\n",
      "Iteration 82, loss = 0.16915645\n",
      "Iteration 83, loss = 0.16963468\n",
      "Iteration 84, loss = 0.16809259\n",
      "Iteration 85, loss = 0.17592820\n",
      "Iteration 86, loss = 0.16900144\n",
      "Iteration 87, loss = 0.16888509\n",
      "Iteration 88, loss = 0.16984617\n",
      "Iteration 89, loss = 0.17002964\n",
      "Iteration 90, loss = 0.16939675\n",
      "Iteration 91, loss = 0.16755909\n",
      "Iteration 92, loss = 0.16846873\n",
      "Iteration 93, loss = 0.16730356\n",
      "Iteration 94, loss = 0.16779507\n",
      "Iteration 95, loss = 0.16849458\n",
      "Iteration 96, loss = 0.16784458\n",
      "Iteration 97, loss = 0.16687141\n",
      "Iteration 98, loss = 0.16699850\n",
      "Iteration 99, loss = 0.16770007\n",
      "Iteration 100, loss = 0.16625213\n",
      "Iteration 101, loss = 0.16929860\n",
      "Iteration 102, loss = 0.16727575\n",
      "Iteration 103, loss = 0.17179434\n",
      "Iteration 104, loss = 0.16691541\n",
      "Iteration 105, loss = 0.16859236\n",
      "Iteration 106, loss = 0.16628817\n",
      "Iteration 107, loss = 0.16599876\n",
      "Iteration 108, loss = 0.16564912\n",
      "Iteration 109, loss = 0.16496186\n",
      "Iteration 110, loss = 0.16515841\n",
      "Iteration 111, loss = 0.16688049\n",
      "Iteration 112, loss = 0.16550747\n",
      "Iteration 113, loss = 0.16543615\n",
      "Iteration 114, loss = 0.16534827\n",
      "Iteration 115, loss = 0.16734194\n",
      "Iteration 116, loss = 0.16438612\n",
      "Iteration 117, loss = 0.16466686\n",
      "Iteration 118, loss = 0.16758817\n",
      "Iteration 119, loss = 0.16589445\n",
      "Iteration 120, loss = 0.16503679\n",
      "Iteration 121, loss = 0.16633430\n",
      "Iteration 122, loss = 0.16434448\n",
      "Iteration 123, loss = 0.16415291\n",
      "Iteration 124, loss = 0.16642219\n",
      "Iteration 125, loss = 0.16420470\n",
      "Iteration 126, loss = 0.16397590\n",
      "Iteration 127, loss = 0.16485849\n",
      "Iteration 128, loss = 0.16689250\n",
      "Iteration 129, loss = 0.16674046\n",
      "Iteration 130, loss = 0.16641102\n",
      "Iteration 131, loss = 0.16478640\n",
      "Iteration 132, loss = 0.16694774\n",
      "Iteration 133, loss = 0.16536537\n",
      "Iteration 134, loss = 0.16298227\n",
      "Iteration 135, loss = 0.16710730\n",
      "Iteration 136, loss = 0.16365175\n",
      "Iteration 137, loss = 0.16597055\n",
      "Iteration 138, loss = 0.16313724\n",
      "Iteration 139, loss = 0.16512181\n",
      "Iteration 140, loss = 0.16317287\n",
      "Iteration 141, loss = 0.16238478\n",
      "Iteration 142, loss = 0.16269726\n",
      "Iteration 143, loss = 0.16388068\n",
      "Iteration 144, loss = 0.16214339\n",
      "Iteration 145, loss = 0.16245572\n",
      "Iteration 146, loss = 0.16195675\n",
      "Iteration 147, loss = 0.16814448\n",
      "Iteration 148, loss = 0.16263480\n",
      "Iteration 149, loss = 0.16379674\n",
      "Iteration 150, loss = 0.16385182\n",
      "Iteration 151, loss = 0.16496398\n",
      "Iteration 152, loss = 0.16311543\n",
      "Iteration 153, loss = 0.16425902\n",
      "Iteration 154, loss = 0.16232838\n",
      "Iteration 155, loss = 0.16179964\n",
      "Iteration 156, loss = 0.16213002\n",
      "Iteration 157, loss = 0.16177620\n",
      "Iteration 158, loss = 0.16242957\n",
      "Iteration 159, loss = 0.16297870\n",
      "Iteration 160, loss = 0.16138669\n",
      "Iteration 161, loss = 0.16242824\n",
      "Iteration 162, loss = 0.16168248\n",
      "Iteration 163, loss = 0.16032915\n",
      "Iteration 164, loss = 0.16225798\n",
      "Iteration 165, loss = 0.16038823\n",
      "Iteration 166, loss = 0.16336596\n",
      "Iteration 167, loss = 0.16293893\n",
      "Iteration 168, loss = 0.16039450\n",
      "Iteration 169, loss = 0.16102100\n",
      "Iteration 170, loss = 0.15990300\n",
      "Iteration 171, loss = 0.16240061\n",
      "Iteration 172, loss = 0.16054958\n",
      "Iteration 173, loss = 0.16083782\n",
      "Iteration 174, loss = 0.16129572\n",
      "Iteration 175, loss = 0.16069681\n",
      "Iteration 176, loss = 0.16004681\n",
      "Iteration 177, loss = 0.16184122\n",
      "Iteration 178, loss = 0.16078488\n",
      "Iteration 179, loss = 0.16024708\n",
      "Iteration 180, loss = 0.16107912\n",
      "Iteration 181, loss = 0.16157217\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "2139\n",
      "0.9457690509583918\n",
      "0.008547008547008548\n",
      "1\n",
      "0.9457690509583918\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, ), random_state=None, max_iter=500, verbose=True).fit(kerasXW2V, kerasYW2V)\n",
    "y_pred = clf.predict(kerasXDevW2V)\n",
    "accuracy(y_pred)\n",
    "\n",
    "print(clf.score(kerasXDevW2V, kerasYDevW2V, sample_weight=None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer()\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2), token_pattern=r'\\b\\w+\\b', min_df=1)\n",
    "\n",
    "XUnigram = vectorizer.fit_transform(kerasX)\n",
    "XPredUnigram = vectorizer.transform(kerasXDev)\n",
    "XBigram = bigram_vectorizer.fit_transform(kerasX)\n",
    "XPredBigram = bigram_vectorizer.transform(kerasXDev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py:688: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.41079023\n",
      "Iteration 2, loss = 0.34481967\n",
      "Iteration 3, loss = 0.20928905\n",
      "Iteration 4, loss = 0.11177316\n",
      "Iteration 5, loss = 0.09402893\n",
      "Iteration 6, loss = 0.08595530\n",
      "Iteration 7, loss = 0.07006549\n",
      "Iteration 8, loss = 0.06156909\n",
      "Iteration 9, loss = 0.05518281\n",
      "Iteration 10, loss = 0.07142498\n",
      "Iteration 11, loss = 0.07134009\n",
      "Iteration 12, loss = 0.04937283\n",
      "Iteration 13, loss = 0.04451961\n",
      "Iteration 14, loss = 0.04167503\n",
      "Iteration 15, loss = 0.03963054\n",
      "Iteration 16, loss = 0.05614215\n",
      "Iteration 17, loss = 0.04829761\n",
      "Iteration 18, loss = 0.03987722\n",
      "Iteration 19, loss = 0.03593728\n",
      "Iteration 20, loss = 0.03379092\n",
      "Iteration 21, loss = 0.03241817\n",
      "Iteration 22, loss = 0.03087199\n",
      "Iteration 23, loss = 0.02979729\n",
      "Iteration 24, loss = 0.02880739\n",
      "Iteration 25, loss = 0.02831421\n",
      "Iteration 26, loss = 0.02741464\n",
      "Iteration 27, loss = 0.02683284\n",
      "Iteration 28, loss = 0.02596933\n",
      "Iteration 29, loss = 0.02548347\n",
      "Iteration 30, loss = 0.02520670\n",
      "Iteration 31, loss = 0.02401970\n",
      "Iteration 32, loss = 0.02337157\n",
      "Iteration 33, loss = 0.02286655\n",
      "Iteration 34, loss = 0.02227438\n",
      "Iteration 35, loss = 0.03300124\n",
      "Iteration 36, loss = 0.13854439\n",
      "Iteration 37, loss = 0.26798417\n",
      "Iteration 38, loss = 0.05233483\n",
      "Iteration 39, loss = 0.03740491\n",
      "Iteration 40, loss = 0.03329867\n",
      "Iteration 41, loss = 0.03127038\n",
      "Iteration 42, loss = 0.03018574\n",
      "Iteration 43, loss = 0.02803112\n",
      "Iteration 44, loss = 0.02684907\n",
      "Iteration 45, loss = 0.02591778\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
      "Best parameters found:\n",
      " {'activation': 'relu', 'alpha': 0.05, 'hidden_layer_sizes': (100,), 'learning_rate': 'constant', 'solver': 'adam', 'verbose': True}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "mlp = MLPClassifier(max_iter=50)\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['tanh', 'relu', 'sigmoid'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "    'verbose': [True]\n",
    "}\n",
    "parameter_space2 = {\n",
    "    'hidden_layer_sizes': [(100,)],\n",
    "    'activation': ['relu'],\n",
    "    'solver': ['adam'],\n",
    "    'alpha': [.05],\n",
    "    'learning_rate': ['constant'],\n",
    "    'verbose': [True]\n",
    "}\n",
    "# clf = GridSearchCV(mlp, parameter_space2, n_jobs=-1, cv=3)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', alpha=.1, random_state=None, max_iter=500, verbose=True).fit(kerasXW2V, kerasYW2V)\n",
    "\n",
    "clf.fit(XUnigram, kerasY)\n",
    "print('Best parameters found:\\n', clf.best_params_)\n",
    "\n",
    "y_pred = clf.predict(XPredUnigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2139\n",
      "0.9504441327723235\n",
      "0.29914529914529914\n",
      "35\n"
     ]
    }
   ],
   "source": [
    "countRight = 0\n",
    "countOf1 = 0\n",
    "\n",
    "print(len(y_pred))\n",
    "\n",
    "for k in range(len(y_pred)):\n",
    "    if y_pred[k] == kerasYDev[k]:\n",
    "        countRight += 1\n",
    "        if kerasYDev[k] == 1:\n",
    "            countOf1 += 1\n",
    "\n",
    "accuracy = countRight/2139\n",
    "homelessAccuracy = countOf1/117\n",
    "\n",
    "print(accuracy)\n",
    "print(homelessAccuracy)\n",
    "print(countOf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9504441327723235\n",
      "0.5932203389830508\n",
      "0.29914529914529914\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(kerasYDev, y_pred)\n",
    "precision = precision_score(kerasYDev, y_pred)\n",
    "recall = recall_score(kerasYDev, y_pred)\n",
    "\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall) # recall is accuracy of homeless notes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.35899351\n",
      "Iteration 2, loss = 0.22965590\n",
      "Iteration 3, loss = 0.21094047\n",
      "Iteration 4, loss = 0.19982007\n",
      "Iteration 5, loss = 0.19122671\n",
      "Iteration 6, loss = 0.18520706\n",
      "Iteration 7, loss = 0.18042573\n",
      "Iteration 8, loss = 0.17641142\n",
      "Iteration 9, loss = 0.17407182\n",
      "Iteration 10, loss = 0.17146092\n",
      "Iteration 11, loss = 0.16785741\n",
      "Iteration 12, loss = 0.16621674\n",
      "Iteration 13, loss = 0.16461316\n",
      "Iteration 14, loss = 0.16237088\n",
      "Iteration 15, loss = 0.15997585\n",
      "Iteration 16, loss = 0.15971386\n",
      "Iteration 17, loss = 0.15677006\n",
      "Iteration 18, loss = 0.15509859\n",
      "Iteration 19, loss = 0.15378213\n",
      "Iteration 20, loss = 0.15377405\n",
      "Iteration 21, loss = 0.15197781\n",
      "Iteration 22, loss = 0.15035555\n",
      "Iteration 23, loss = 0.14932006\n",
      "Iteration 24, loss = 0.14857442\n",
      "Iteration 25, loss = 0.14697457\n",
      "Iteration 26, loss = 0.14589999\n",
      "Iteration 27, loss = 0.14515216\n",
      "Iteration 28, loss = 0.14452872\n",
      "Iteration 29, loss = 0.14374397\n",
      "Iteration 30, loss = 0.14245342\n",
      "Iteration 31, loss = 0.14103111\n",
      "Iteration 32, loss = 0.14060200\n",
      "Iteration 33, loss = 0.13946681\n",
      "Iteration 34, loss = 0.13754773\n",
      "Iteration 35, loss = 0.13628514\n",
      "Iteration 36, loss = 0.13623095\n",
      "Iteration 37, loss = 0.13546568\n",
      "Iteration 38, loss = 0.13541253\n",
      "Iteration 39, loss = 0.13366984\n",
      "Iteration 40, loss = 0.13318628\n",
      "Iteration 41, loss = 0.13497898\n",
      "Iteration 42, loss = 0.13160003\n",
      "Iteration 43, loss = 0.12926254\n",
      "Iteration 44, loss = 0.12926344\n",
      "Iteration 45, loss = 0.12886694\n",
      "Iteration 46, loss = 0.12726252\n",
      "Iteration 47, loss = 0.12755022\n",
      "Iteration 48, loss = 0.12720039\n",
      "Iteration 49, loss = 0.12427670\n",
      "Iteration 50, loss = 0.12284227\n",
      "Iteration 51, loss = 0.12225900\n",
      "Iteration 52, loss = 0.12074592\n",
      "Iteration 53, loss = 0.12255572\n",
      "Iteration 54, loss = 0.11931457\n",
      "Iteration 55, loss = 0.12032909\n",
      "Iteration 56, loss = 0.12313894\n",
      "Iteration 57, loss = 0.11644529\n",
      "Iteration 58, loss = 0.11462683\n",
      "Iteration 59, loss = 0.11568867\n",
      "Iteration 60, loss = 0.11410054\n",
      "Iteration 61, loss = 0.11469255\n",
      "Iteration 62, loss = 0.11336322\n",
      "Iteration 63, loss = 0.11158683\n",
      "Iteration 64, loss = 0.11127500\n",
      "Iteration 65, loss = 0.10983862\n",
      "Iteration 66, loss = 0.10803248\n",
      "Iteration 67, loss = 0.10799753\n",
      "Iteration 68, loss = 0.10672798\n",
      "Iteration 69, loss = 0.10770588\n",
      "Iteration 70, loss = 0.11115797\n",
      "Iteration 71, loss = 0.10558960\n",
      "Iteration 72, loss = 0.10510475\n",
      "Iteration 73, loss = 0.10401614\n",
      "Iteration 74, loss = 0.10347465\n",
      "Iteration 75, loss = 0.10491216\n",
      "Iteration 76, loss = 0.10239266\n",
      "Iteration 77, loss = 0.10612297\n",
      "Iteration 78, loss = 0.10434249\n",
      "Iteration 79, loss = 0.10733012\n",
      "Iteration 80, loss = 0.10016907\n",
      "Iteration 81, loss = 0.09906222\n",
      "Iteration 82, loss = 0.10054425\n",
      "Iteration 83, loss = 0.09838337\n",
      "Iteration 84, loss = 0.09666027\n",
      "Iteration 85, loss = 0.09674488\n",
      "Iteration 86, loss = 0.09618278\n",
      "Iteration 87, loss = 0.09550746\n",
      "Iteration 88, loss = 0.09832232\n",
      "Iteration 89, loss = 0.10308639\n",
      "Iteration 90, loss = 0.09586943\n",
      "Iteration 91, loss = 0.09535442\n",
      "Iteration 92, loss = 0.09422395\n",
      "Iteration 93, loss = 0.09403702\n",
      "Iteration 94, loss = 0.09229437\n",
      "Iteration 95, loss = 0.09273037\n",
      "Iteration 96, loss = 0.09135042\n",
      "Iteration 97, loss = 0.09045203\n",
      "Iteration 98, loss = 0.09138426\n",
      "Iteration 99, loss = 0.09018122\n",
      "Iteration 100, loss = 0.09224453\n",
      "Iteration 101, loss = 0.09139187\n",
      "Iteration 102, loss = 0.09115181\n",
      "Iteration 103, loss = 0.08833317\n",
      "Iteration 104, loss = 0.09066089\n",
      "Iteration 105, loss = 0.08927377\n",
      "Iteration 106, loss = 0.08787961\n",
      "Iteration 107, loss = 0.08951659\n",
      "Iteration 108, loss = 0.08939229\n",
      "Iteration 109, loss = 0.09119164\n",
      "Iteration 110, loss = 0.08861741\n",
      "Iteration 111, loss = 0.10024463\n",
      "Iteration 112, loss = 0.09016107\n",
      "Iteration 113, loss = 0.08825227\n",
      "Iteration 114, loss = 0.08571805\n",
      "Iteration 115, loss = 0.09303484\n",
      "Iteration 116, loss = 0.08710899\n",
      "Iteration 117, loss = 0.08431085\n",
      "Iteration 118, loss = 0.08492426\n",
      "Iteration 119, loss = 0.08458889\n",
      "Iteration 120, loss = 0.08438659\n",
      "Iteration 121, loss = 0.08574511\n",
      "Iteration 122, loss = 0.08473736\n",
      "Iteration 123, loss = 0.08441798\n",
      "Iteration 124, loss = 0.08402200\n",
      "Iteration 125, loss = 0.08570294\n",
      "Iteration 126, loss = 0.08408717\n",
      "Iteration 127, loss = 0.08304322\n",
      "Iteration 128, loss = 0.08420659\n",
      "Iteration 129, loss = 0.08504261\n",
      "Iteration 130, loss = 0.08493237\n",
      "Iteration 131, loss = 0.08370545\n",
      "Iteration 132, loss = 0.08262726\n",
      "Iteration 133, loss = 0.08113233\n",
      "Iteration 134, loss = 0.08265557\n",
      "Iteration 135, loss = 0.08028705\n",
      "Iteration 136, loss = 0.08018993\n",
      "Iteration 137, loss = 0.08142243\n",
      "Iteration 138, loss = 0.08093375\n",
      "Iteration 139, loss = 0.07976133\n",
      "Iteration 140, loss = 0.08059010\n",
      "Iteration 141, loss = 0.08157872\n",
      "Iteration 142, loss = 0.08070300\n",
      "Iteration 143, loss = 0.07947121\n",
      "Iteration 144, loss = 0.08045791\n",
      "Iteration 145, loss = 0.07938993\n",
      "Iteration 146, loss = 0.08130576\n",
      "Iteration 147, loss = 0.07981787\n",
      "Iteration 148, loss = 0.08089928\n",
      "Iteration 149, loss = 0.08063907\n",
      "Iteration 150, loss = 0.08063423\n",
      "Iteration 151, loss = 0.08568153\n",
      "Iteration 152, loss = 0.08541463\n",
      "Iteration 153, loss = 0.07972693\n",
      "Iteration 154, loss = 0.08101991\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(100, ), activation='relu', alpha=.05, random_state=None, max_iter=500, verbose=True)\n",
    "\n",
    "clf.fit(X, kerasY)\n",
    "y_pred = clf.predict_proba(Xpred)\n",
    "y_test = clf.predict_proba(Xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9513791491351099\n",
      "0.5643564356435643\n",
      "0.48717948717948717\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict_proba(Xpred)\n",
    "\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "predictions = []\n",
    "for i in y_pred:\n",
    "    if i[0] > .75:\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)\n",
    "\n",
    "accuracy = accuracy_score(kerasYDev, predictions)\n",
    "precision = precision_score(kerasYDev, predictions)\n",
    "recall = recall_score(kerasYDev, predictions)\n",
    "\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEWCAYAAAB42tAoAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAA9iElEQVR4nO3dd3RU5fbw8e+WDiIoTQURFERAioIgihUL2Lkqwa5XL3bsveB7sWAXFUREf3jVS6KIXBuKDURRERuGIiIiRFBpIlUJ2e8f+wwZwmQyCTlTkv1ZKyuZmTPnPHOSzJ6nnL1FVXHOOeeKs12qG+Cccy69eaBwzjkXlwcK55xzcXmgcM45F5cHCuecc3F5oHDOOReXB4oKTkRmishhqW5HuhCRW0RkVIqOPVpE7krFscubiJwpIhPL+Nwy/02KyCcism9ZnltWIjJQRIYk85jpxgNFEonIAhFZLyJrROTX4I1j+zCPqartVXVSmMeIEJEaInKviCwMXucPInK9iEgyjh+jPYeJSF70fap6j6peGNLxJHhTyRWRtSKSJyIvi0iHMI5XViJyp4i8sC37UNUXVfXoBI61VXAs69+kiJwArFbVr4Pbd4rIxuD/6Q8RmSoiPYo8p76IPBn8v60Tke9E5PwY+z5DRKYH+1oiIhNEpGfw8EjgLBFpXNo2VxQeKJLvBFXdHugM7AvcnNrmlJ6IVC3moZeBXsCxQF3gbGAAMDSENoiIpNvf71DgSmAgsBOwFzAeOK68DxTndxC6FB77YuD5IvflBP9PDYEPsb9BAESkOvAesDvQA6gHXA8MEZFrora7BngUuAdoAjQHhgMnAajqBmACcE4YLyojqKp/JekLWAAcGXX7fuDNqNsHAFOBP4BvgcOiHtsJ+D9gMbASGB/12PHAN8HzpgIdix4T2BVYD+wU9di+wDKgWnD7n8DsYP/vALtHbavAZcAPwE8xXlsvYAOwW5H7uwObgFbB7UnAvcA0YBXwvyJtincOJgF3A58Er6UVcH7Q5tXAfOCiYNs6wTYFwJrga1fgTuCFYJsWwes6F1gYnItbo45XC3guOB+zgRuAvGJ+t62D19ktzu9/NDAMeDNo7+fAnlGPDwUWAX8CXwIHRz12JzAWeCF4/EKgG/BpcK6WAE8A1aOe0x54F1gB/AbcAvQG/gY2Bufk22DbesAzwX5+Ae4CqgSPnRec80eCfd0V3Pdx8LgEj/0e/E5nAPtgHxI2BsdbA7xe9P8AqBK068fgnHxJkb+hYLvqwe+zWZFz8kLU7XbB77NRcPuCoE11iuwrK2jPDsHrXgOcVsL/7pnAh6l+D0nVV8obUJm+ivyDNAO+A4YGt5sCy7FP49sBRwW3I3/0bwI5wI5ANeDQ4P79gn+G7sE/3bnBcWrEOOYHwL+i2vMAMCL4+WRgHtAWqArcBkyN2laDN52dgFoxXtsQYHIxr/tnCt/AJwVvRPtgb+avUPjGXdI5mIS9obcP2lgN+7S+J/ZmdSiwDtgv2P4wiryxEztQPI0FhU7AX0Db6NcUnPNm2BtgcYHiYuDnEn7/o7E32m5B+18EsqMePwtoEDx2LfArUDOq3RuD39N2QXu7YIG1avBaZgNXBdvXxd70rwVqBre7Fz0HUcceDzwV/E4aY4E88js7D8gHrgiOVYstA8Ux2Bt8/eD30BbYJeo13xXn/+B67P+gTfDcTkCDGOeuPbA2zu+yevD7WgZUDe7LBp6Lsa+qwes5Bguc+ZHnxPnd7QesSPV7SKq+0q3rXhmMF5HV2CfH34FBwf1nAW+p6luqWqCq7wLTgWNFZBegD3Cxqq5U1Y2qOjl43r+Ap1T1c1XdpKrPYW92B8Q49n+B08GGboD+wX0AFwH3qupsVc3HuuGdRWT3qOffq6orVHV9jH03xN6YYlkSPB7xvKrmqupa4Hagn4hUiXcOop47WlVnqmp+cB7eVNUf1UwGJgIHF9OO4vw/VV2vqt9ivZhOwf39gHuCc54HPBZnHw3ivP5o41R1WnCOX8SGIAFQ1RdUdXnw2h4CamBvoBGfqur44NysV9UvVfWzYPsF2Bv9ocG2xwO/qupDqrpBVVer6uexGiQiTbC/r6tUda2q/o71EPpHbbZYVR8PjlX0978RC0R7AxL8DSVyLsB6Rrep6vfB7/BbVV0eY7v6WI+jqH4i8gfW2/gXcGpwbqGYv8ng8WXB4w2AZVHPKc5qrPdRKXmgSL6TVbUu9ml3bwrfQHcHTgsm5f4I/vh7ArsAu2GfZlbG2N/uwLVFnrcbNsxS1Figh4jsChyCfZqeErWfoVH7WIF9wmsa9fxFcV7XsqCtsewSPB5rPz9jPYOGxD8HMdsgIn1E5DMRWRFsfyxbBqVE/Br18zogssBg1yLHi/f6l1P860/kWIjItSIyW0RWBa+lHlu+lqKvfS8ReSOYqP0TC+6R7XfDhnMSsTv2O1gSdd6fwnoWMY8dTVU/wIa9hgG/ichIEdkhwWMn2s6VWDAq6iVVrY/NLeRivayImH+TwRxLw+Dx5UDDBOZd6mLDapWSB4oUCT79jgYeDO5ahH3Srh/1VUdVhwSP7SQi9WPsahFwd5Hn1VbVMTGO+Qf2ibsfcAYwRlU1aj8XFdlPLVWdGr2LOC/pPaC7iOwWfaeIdMPeDD6Iujt6m+bYJ9JlJZyDrdogIjWwoasHgSbBG8ZbWIArqb2JWIINOcVqd1HvA81EpGtZDiQiBwM3Yr+bHYPXsorC1wJbv54ngTlAa1XdARvrj2y/CBuSi6XofhZhvdCGUed9B1VtH+c5W+5Q9TFV7YINEe2FDSmV+LwS2hntB6wj3DTWg6q6DOsV3xn0wMH+JvuISJ0im5+Cvd7PsDmeDdiQXjxtsd5mpeSBIrUeBY4Skc7YJOUJInKMiFQRkZrB8s5mQTd+AjBcRHYUkWoickiwj6eBi0Wke7ASqI6IHCcisT59gQ01nYP9s/w36v4RwM0i0h5AROqJyGmJvhBVfQ97s3xFRNoHr+EAbHjlSVX9IWrzs0SknYjUBv4NjFXVTfHOQTGHrY4NzywF8kWkDxC9ZPM3oIGIlHXI4CXsnOwYvEFdXtyGwesbDowJ2lw9aH9/EbkpgWPVxcbKlwJVReQObLK1pOf8CawRkb2BS6IeewPYWUSuElu2XFdEugeP/Qa0iKwaC/6+JgIPicgOIrKdiOwpIoeSABHZP/j7qwasxd54N0Uda484Tx8FDBaR1sHfb0cRaVB0I1XdiL3xF9smVZ2DLcK4IbjreSAPeFlEWgT/N8dgQ4h3quoqVV0F3AEME5GTRaR2sF0fEbk/aveHYv+DlZIHihRS1aXAf4DbVXURthzvFuzNYhH2qSzyOzob++Q9B5vbuCrYx3RsbPYJrHs+D5toLM5r2Aqd34Ix+UhbXgXuA7KDYYxcbNy6NE7Blii+ja0keQFbSXNFke2ex3pTv2ITrQODNpR0DragqquD576EvfYzgtcXeXwOMAaYHwypxBqOi+ff2BvNT9ib1Fjsk2hxBlI4BPMHNqTSF3g9gWO9g70RzcWG4zYQf6gL4DrsNa/GPjDkRB4Izs1RwAnYef4BODx4OLKEdLmIfBX8fA4WeGdh53IsiQ2lgQW0p4Pn/YwN50R6ys8A7YLzPz7Gcx/Gfn8TsaD3DDZZHstT2P9BPA8AA0Sksar+ha34W4StMPszON6tqvpA5Amq+jBwDbaAI/J3dzk2wY+I1MSGNJ8r4dgVlhSOPDgXPhGZhK1UScnV0dtCRC4B+qtqQp+0XfkTkY+BKzS46C5Jx7wCW7J7Q4kbV1Apu2jHuXQXjHXvgY1jt8aWmj6R0kZVcqras+Styv2Yjyf7mOkmtKEnEXlWRH4XkdxiHhcReUxE5onIDBHZL6y2OFdG1bHhjtXYZPz/sHkI5yqV0IaegsnWNcB/VHWfGI8fi41dH4tdLDZUVbsX3c4551xqhdajUNWPsLX4xTkJCyKqqp8B9aOWtTnnnEsTqZyjaMqWqzrygvu2upJSRAZgeWOoU6dOl7333jspDXTOuUy1ejWsXAnVly+hccGvfE3BMlVtVJZ9pTJQxEo9HXMcTFVHYql+6dq1q06fPj3MdjnnXMZRhc8+g5wcePllWLxYqVVLGNTjNU6tO5FWbw/7uaz7TmWgyGPLK12bYZlRnXPOJUAVvvzSgsNLL8HChdCk+kpe2Pk6Gvfbgz2euZXttz8ROBFkWJmPk8oL7l4DzglWPx0ArCpFIjHnnKuUVGHGDLj1VmjdGvbfHx59FDp0gA8HvsriHdtx5C/P0bHtRrYvp7JoofUoRGQMlviuoViVsUFY4jFUdQSWk+dY7EridVhdAeecczHMnm09h5wcmDMHqlSBI46Am2+Gfxz0GzvecQU89jJ07gxvvQn7ld8VB6EFClU9vYTHI4VwnHPOxfDjjxYYsrPhu+9ABA49FK68Ek45BRpFpqanL4I334S774brr4dq1cq1HX5ltnPOpZGff7b5hpwcm38AOPBAGDoUTj0Vdt01asMnXofLL4euXW2CosFW+RTLhQcK55xLsV9+sZVKOTm2cgls7uHBB+G006B586iNCwrgySfhpiAp8SmnwC67hBYkwAOFc86lxO+/w9ixFhymTLFJ6k6d4J57oF8/2DNWlY7vv4cLL4SPP4ZjjoGnnrIgETIPFM45lyTLl8O4cRYcPvzQOgdt28Kdd0JWFrRpE+fJ69ZBz56waROMHg3nnGOTFknggcI550K0ahWMH2/B4d13IT8fWrWy1UpZWbDPPiW838+da+tga9eG55+3VU0775yk1hsPFM45V87WrIHXXrPg8Pbb8PffsPvucM01Fhz23TeBzsCGDTB4MNx3n/UgzjoLevdORvO34oHCOefKwbp18NZbFhzeeMPe55s2hUsvhf79oVu3UowUffIJXHCBzUmcfz4cd1yobS+JBwrnnCujv/6yHkNOjvUg1q6Fxo3tPT4rCw46CLYrbf6LwYNh0CBb6vTOO3D00SU/J2QeKJxzrhQ2boT33rPgMH68zUHstBOceaYFh0MPtaumS03VuhydO8MVV9jFc+WVg2MbeaBwzrkS5OfDpEkWHMaNgxUroF496NvXgkOvXttwMfSKFXD11TbDffvtcMIJ9pVGPFA451wMBQV2uUJOjl3v8Pvv9gH/xBNtzuHoo6FGjW08yNixcNllFixuv71c2h0GDxTOORfYuqYD1KoFxx9vPYdjj7Xb22zJEku9MW4cdOkCEyfa1XZpygOFc65SU4WvvirMzLpwIVSvDn36WM/h+ONDmCpYvNgmqu+7z9bMVk3vt+L0bp1zzoVA1bKxRoLDjz/ae/XRR9uio5NOsjmIcrVgAbz+uk1Ud+kCixbBjjuW80HC4YHCOVdpFK3psN12NhF98802Mb3TTiEcdNMmGDYMbrnFDnjaaXZldYYECfBA4Zyr4CI1HXJyrDKcCBxyCAwcaIlXGzcO8eCzZ1sSv6lT7arqp55KevqN8uCBwjlX4SRc0yFM69ZZRCoogP/8x1JwJCmJX3nzQOGcqxAWLy6s6fDpp3Zf167wwAOWtnuLmg5hmjPH0sDWrg0vvmirmZo0SdLBw1Hai8udcy5t/P47DB9uV0M3awZXXWUf5O+5B+bNgy++gOuuS1KQWL8ebrwR2re3AAE2O57hQQK8R+GcyzArVhTWdPjgg8KaDoMG2bUOe++dgkZ99JHNRfzwg30//vgUNCI8Hiicc2lvm2s6hOn//T+rPNSypSWB6tUrRQ0JjwcK51xaWrPGLjvIyYEJEwprOlx9tQWH/fZL8dxwJIlf167WqMGDoU6dFDYoPB4onHNpY/16ePNNCw5vvmm3d93VajpkZUH37mmwcGjZMgsMrVvDHXdYrYgU14sImwcK51xK/fWXZbOI1HRYs8aubTj/fAsOPXuWoaZDGFRtWdXll8PKlTYpUkl4oHDOJV1xNR1OP72wpkNapT9avNi6Nf/7nw01vfcedOyY6lYlTTr9KpxzFVh+PkyebMHhlVcKazqcfLIl39ummg5h+/VXW2L1wAO2Bjetolj4Kterdc4lVbyaDllZcMwx5VDTISzz59tY2FVX2cz5woVQv36qW5USHiicc+VKFT7/HLKzQ67pEJZNm+Cxx+DWW62L07+/5WeqpEECPFA458pBdE2Hl16yXEuRmg5ZWVbZM03KP8c3cyZccIFFuuOOgxEjMjKJX3nzQOGcK5Pomg4vvWQpMyI1Hf7975BqOoRp3TqbRReB//7XehIpX4ubHjxQOOdKZc6cwrTds2fb0tUjjrA0R337QoMGqW5hKc2aZTlAate28bJOnaBRo1S3Kq14oHDOlShWTYeDD7ZibaHXdAjLunV2LcTDD8Po0XD22XDkkaluVVryQOGci2nhwsKaDtOn2309esCjj1qRtqTUdAjLpEnwr3/ZeNlFF9kyLFcsDxTOuc1i1XTo0sUuHzjtNMu1lPEGDbJJlD33tGsjDj881S1Kex4onKvkfv/dLoDLybFs2ap20fHdd1vBn1atUt3CchJJ4tetG1x7rQWL2rVT3aqMEGqgEJHewFCgCjBKVYcUebwe8ALQPGjLg6r6f2G2yTkXu6bD3nunuKZDWJYuhSuvtKpzgwZViiR+5S20QCEiVYBhwFFAHvCFiLymqrOiNrsMmKWqJ4hII+B7EXlRVf8Oq13OVVarVlmqopwcmDjRUmrsuSfcdJMFhw4dKthqUFUYMwYGDoQ//7S6Ea5MwuxRdAPmqep8ABHJBk4CogOFAnVFRIDtgRVAfohtcq5SiVXToXnzNKrpEJa8PLjkEnjjDctN/swzVqLUlUmYgaIpsCjqdh7Qvcg2TwCvAYuBukCWqhYU3ZGIDAAGADRPWoV05zLT+vXw1lsWHN54o7CmwyWX2DVkaVHTIWxLl9qEy8MPW4+iSpVUtyijhRkoYv0papHbxwDfAEcAewLvisgUVf1ziyepjgRGAnTt2rXoPpyr9DKmpkOY5s2z7tPVV8O++8KiRbDDDqluVYUQZqDIA3aLut0M6zlEOx8YoqoKzBORn4C9gWkhtsu5CmHjRnj/fbuYOCNqOoQlP98u7rj9dktFe8YZ0KSJB4lyFOaf0RdAaxFpCfwC9AfOKLLNQqAXMEVEmgBtgPkhtsm5jLZpk10rlpNjq5aWL7f3w759LTgceWQa13QIw3ffWRK/L76wi+aGD7cg4cpVaIFCVfNF5HLgHWx57LOqOlNELg4eHwEMBkaLyHfYUNWNqrosrDY5l4li1XSoU2fLmg41a6a6lSmwbp1dLLfddtat6tevEky+pEaoHVNVfQt4q8h9I6J+XgwcHWYbnMtEkZoOOTl2pfQvv1gwiK7pUGmvFcvNtRVMtWvbCerUCRo2THWrKrTKMILpXEYorqZD796WQiNjajqEZe1am4d49FF47jlL4terV6pbVSl4oHAuhVTtA3IkM2ukpsNRR9n1YSedVKkLqxV6/31L4vfTT3DppXZiXNJ4oHAuBWLVdDj88Ayu6RCm22+Hu+6C1q1h8mQ45JBUt6jS8UDhXJLMn18YHL79trCmw+WXW00HX6xTREGBRdADD4QbboA770zzYtsVlwcK50IUr6bDqadC06YpbV56+v13u5q6TRsbf+vTx75cynigcK6cLVlSWNNh6lS7r0sXuP9+W8FZIWo6hEEVXnzRMr2uWWNpwF1a8EDhXDmoNDUdwrJoEVx8sSWp6tEDRo2Cdu1S3SoX8EDhXBmtWAGvvlpY02HTJqvjcMcddq1D27apbmEGWb4cPvkEhg6Fyy7zJH5pxgOFc6UQq6bDHnvYaqUKWdMhTHPnWgbD666Dzp2tV1G3bqpb5WLwQOFcCdassXTd2dnw9tuWqbV5c7jqKgsOXbp4cCiV/Hx46CGrNlerll0416SJB4k05oHCuRhi1XTYZRcbRs/KspoOFT5tdxi+/Rb++U+7BL1vXxg2zNcFZwAPFM4FYtV0aNQIzjuvsKaDD51vg3XrLOVG1aqW3fCUU1LdIpcgDxSuUovUdMjJsYnpSE2H/v0tOBx2WCWp6RCmGTNs8qZ2bVs33KmTnWSXMfxfwFU6xdV0OPlkCxCVrqZDWNasgVtvhccfh9Gj4ZxzLE+JyzgeKFylUFBgqy8jNR1++81rOoTq3XdhwABYsMBylPTtm+oWuW3ggcJVWKowbZqtVoqu6XDccdZzqNQ1HcJ0661wzz2WgmPKFJvccRkt4UAhInVUdW2YjXFuW6nC118X1nRYsKCwpsP991tNB1+FGZJIEr+ePeHmm+3KQ++mVQglBgoRORAYBWwPNBeRTsBFqnpp2I1zLlG5udZziK7pcOSRlnDUazqE7NdfbXipXTvLz+RJ/CqcRHoUjwDHAK8BqOq3IuIJ4V3Kff99YdruWbMKazrccAP84x9e0yF0qlZp7pprbOnrAQekukUuJAkNPanqItny0tNN4TTHufhi1XTo2dOu2/KaDkn08882WT1xov0CRo2yOQlXISUSKBYFw08qItWBgcDscJvlXKFFiwprOnzxhd13wAHwyCNw2mle0yEl/vjDfhlPPAGXXOKXqVdwiQSKi4GhQFMgD5gI+PyEC1Wsmg777ec1HVLq++/tkvXrr7eL5hYuhO23T3WrXBIkEijaqOqZ0XeIyEHAJ+E0yVVWS5daTYfs7MKaDh06WLnkrCyv6ZAyGzfCgw9atbk6deDcc6FxYw8SlUgigeJxYL8E7nOu1LymQ5r7+mu44AL7fuqpNtTUuHGqW+WSrNhAISI9gAOBRiJyTdRDOwCeGs2VWXRNh3fftQ+se+xhq5WysqwynKftTgPr1sFRR1k+k1desaVkrlKK16Oojl07URWIvkTpT+DUMBvlKp61a+H11y04TJhQWNPhyiu9pkPa+fprKyRUu7blO+nUCXbcMdWtcilUbKBQ1cnAZBEZrao/J7FNroIorqbDRRdZCg2v6ZBmVq+2K6qHDbPrI845x9LnukovkTmKdSLyANAe2Hw9vqoeEVqrXMb66y9bWp+TY8NLXtMhQ7z9tkXwRYusm+fDTC5KIoHiRSAHOB5bKnsusDTMRrnMsnGjTURnZxfWdNhxRwsM/ft7TYe0d/PNMGSIrRz45BPo0SPVLXJpJpF/3waq+oyIXBk1HDU57Ia59LZpE0yebD2HV17ZsqZDVpblWapePdWtdHFt2mTdu0gkv+02qFEj1a1yaSiRQLEx+L5ERI4DFgPNwmuSS1cFBXbxW3b2ljUdTjjBgkPv3p4sNCMsWQKXXQbt28PgwVaM45hjUt0ql8YSCRR3iUg94Frs+okdgKvCbJRLH5GaDpG03dE1HbKy7LvXdMgQqlZp7pprYMMGrxPhElZioFDVN4IfVwGHw+Yrs10FFaumQ7VqXtMhoy1YAP/6F7z3Hhx8sCXx22uvVLfKZYh4F9xVAfphOZ7eVtVcETkeuAWoBeybnCa6ZMnNLczM+sMPhTUdBg2yuQev6ZDBVq2Cr76C4cNtdZOvS3alEK9H8QywGzANeExEfgZ6ADep6vhEdi4ivbGEglWAUao6JMY2hwGPAtWAZap6aOLNd9sqVk2Hww6zvG9e0yHDzZplSfxuuqkwiV+dOqlulctA8QJFV6CjqhaISE1gGdBKVX9NZMdBj2QYcBSWdfYLEXlNVWdFbVMfGA70VtWFIuJJZJJg/nwbUsrO3rKmwxNPWDofr+mQ4f7+28YIBw+2McJ//tPyM3mQcGUUL1D8raoFAKq6QUTmJhokAt2Aeao6H0BEsoGTgFlR25wBjFPVhcFxfi9V613CvKZDJTF9uiXxmzHDLmIZOtST+LltFi9Q7C0iM4KfBdgzuC2AqmrHEvbdFFgUdTsP6F5km72AaiIyCcsnNVRV/1N0RyIyABgA0Lx58xIO6yKKq+lw331W06FFi5Q2z5W3tWttmWvNmnZZ/IknprpFroKIFyi2NcFzrBRvGuP4XYBe2AT5pyLymarO3eJJqiOBkQBdu3Ytug8XJVLTISfHLohThX32sZoO/fpB69apbqErd199ZUn86tSxS+M7dvSVB65cxUsKuK2JAPOwyfCIZtjFekW3Waaqa4G1IvIR0AmYi0tYrJoObdrA7bfbtQ7t2qW6hS4Uf/5pE9VPPlmYxO+QQ1LdKlcBhZmB5wugtYi0BH4B+mNzEtH+BzwhIlWxtObdgUdCbFOF8eefhTUdJk60fEstW3pNh0rjrbdsmevixXYB3SmnpLpFrgILLVCoar6IXA68gy2PfVZVZ4rIxcHjI1R1toi8DcwACrAltLlhtSnTrV1r6bqzswtrOuy2GwwcaMGha1cPDpXCjTfaqqZ27SyXSveiU3/Ola+EAoWI1AKaq+r3pdm5qr4FvFXkvhFFbj8APFCa/VYm69dbUIjUdFi3rrCmQ1aWrVzya6cqAVVLtlWlCvTqZRPWt9ziSfxcUpQYKETkBOBBbGiopYh0Bv6tqr6kIiR//23DSdnZhTUdGja0IeisLMvA4DUdKpFffoFLL4UOHWxVwtFH25dzSZJIj+JO7JqISQCq+o2ItAivSZVTpKZDTo5NTP/xR2FNh6wsOPxwr+lQ6ahaTqbrrrNPD4cfnuoWuUoqkbeefFVdJT74Xe5i1XSoW9fyKvXv7zUdKrWffrIL5z780HKqPP00tGqV6la5SiqRQJErImcAVUSkNTAQmBpusyquSE2HnBybh/z1V0vTfeKJXtPBRVmzxq6ufuopuPBCn4hyKZVIoLgCuBX4C/gvtorprjAbVdFE13R4+WXIy7NgcOyx1nPwmg4OsPS9r71mk9QdOlgSP//DcGkgkUDRRlVvxYKFS5AqfPNNYWbW6JoOQ4ZYD8JrOjjA5h/uvRfuvhvq1bMeROPGHiRc2kgkUDwsIrsALwPZqjoz5DZltKI1HapUsbmGO+6wuYcdd0x1C11a+eILy+6amwtnnAGPPgqNGqW6Vc5tIZEKd4eLyM5YEaORIrIDkKOqPvwUKK6mw3XXWU2Hhg1T3UKXltautS5mrVo25HTCCalukXMxJbTgMkgv/piIfAjcANxBJZ+n+OmnwuDwzTd2X8+e8PjjVtNh551T2jyXzqZPtzS+derYhTIdOtiQk3NpKpEL7toCWcCpwHIgG7g25HalpVg1Hbp3h4cftpoOzZqltn0uza1aZcm4Ro4sTOLXs2eqW+VciRLpUfwfMAY4WlWLZn+t8JYssWWsOTnwySd23777ek0HV0qvvw4XX2zroa+7zrqdzmWIROYoDkhGQ9LJsmV2AVx29pY1HQYPtmsdvKaDK5Xrr4cHH7QhpvHjYf/9U90i50ql2EAhIi+paj8R+Y4tCw4lWuEuo6xcWVjT4f33vaaD20aq9kdUtarlZdphB8v66pfauwwUr0dxZfD9+GQ0JBX+/NMWm2Rnb1nT4frrLTh06uRpu10Z5OXBJZdYUZC774ajjrIv5zJUvAp3S4IfL1XVG6MfE5H7gBu3flb6i9R0yMmx2i9e08GVm4ICy8l0/fXWm/AMr66CSGQy+yi2Dgp9YtyXtmLVdNh5ZxgwwFJoeE0Ht83mz7cL5yZPtnoRI0fCHnukulXOlYt4cxSXAJcCe4jIjKiH6gKfhN2wbRWp6ZCTY0vVV6/2mg4uRGvX2tWWo0ZZwPBuqatA4vUo/gtMAO4Fboq6f7Wqrgi1VWUUq6ZD/fp2jUNWFhxxhNd0cOXou+/sU8htt9mKpp9/tqusnatg4r1tqqouEJHLij4gIjulW7CYPx8OPBB++62wpkNWls0h+kITV67++ssmqe+915J3DRhgSfw8SLgKqqQexfHAl9jy2Oi+tAJpNQA7Y4YFiWHDrOfvNR1cKD77zAoKzZoFZ58NjzwCDRqkulXOhSreqqfjg+8tk9ecslu50r4fe6wHCReStWuteEidOrZkrk+fVLfIuaQoca2PiBwkInWCn88SkYdFpHn4TSudFcFA2E47pbYdrgL6/HNb+lqnjqXimDnTg4SrVBJZFPoksE5EOmGZY38Gng+1VWWwYoWtYvJiQK7c/PGHFRE64AB44QW778AD/Y/MVTqJBIp8VVXgJGCoqg7FlsimlZUrbV7RVyW6cjF+vOVtGT3aUm+cdlqqW+RcyiSyWHS1iNwMnA0cLCJVgGrhNqv0VqzwYSdXTq65xiapO3WyoaYuXVLdIudSKpFAkQWcAfxTVX8N5iceCLdZpRfpUThXJtFJ/I491lYy3XCDFTp3rpIrcegpqG73IlBPRI4HNqjqf0JvWSl5j8KV2cKFtppp0CC7feSRcOutHiScCySy6qkfMA04Daub/bmIpF3VFQ8UrtQKCmD4cGjf3nI07bprqlvkXFpKZOjpVmB/Vf0dQEQaAe8BY8NsWGn50JMrlXnz7MrMKVPs8v2RI71coXPFSCRQbBcJEoHlJLZaKmkKCmwlo/coXMI2bIC5c+H//g/OPdeXyzkXRyKB4m0ReQermw02uf1WeE0qvVWrbC7SexQurm++sSR+gwZZbdsFC/wyfucSkMhk9vXAU0BHoBMwsmgho1Tzq7JdXBs22OR0167w5JPwe9BB9iDhXELi1aNoDTwI7Al8B1ynqr8kq2Gl4YHCFWvqVEviN2eODTE9/LD/oThXSvF6FM8CbwCnYBlkH09Ki8ogkhDQh57cFtauhRNOsJKGb79tV1l7kHCu1OLNUdRV1aeDn78Xka+S0aCy8B6F28Knn0L37pbE7403bD7C8zM5V2bxehQ1RWRfEdlPRPYDahW5XSIR6S0i34vIPBG5Kc52+4vIprJen+E9CgfYH8I//2mJ+54P8lb26OFBwrltFK9HsQR4OOr2r1G3FTgi3o6DnFDDgKOAPOALEXlNVWfF2O4+4J3SNb1QpEfhgaISGzcOLrsMli6Fm2+28obOuXIRr3DR4du4727APFWdDyAi2VgG2llFtrsCeAXYv6wHWrHCRhlq1CjrHlxGu/pqePRR6NzZCgrtu2+qW+RchZLIdRRl1RRYFHU7D+gevYGINAX6Yr2TYgOFiAwABgA0b751zSS/KrsSik7id/zxVrP6uus8P5NzIQjzCutYl7pqkduPAjeq6qZ4O1LVkaraVVW7NmrUaKvHPc9TJbNgAfTuDbffbrd79bLhJg8SzoUizECRB+wWdbsZsLjINl2BbBFZAJwKDBeRk0t7oBUrvEdRKRQUwOOP2yqmqVNh991T3SLnKoVEssdKUCv7juB2cxHplsC+vwBai0hLEakO9Adei95AVVuqagtVbYElGbxUVceX9kWsXOk9igrvhx/gkENg4EA4+GDIzYWLL051q5yrFBLpUQwHegCnB7dXY6uZ4lLVfOBybDXTbOAlVZ0pIheLSLn+h/vQUyXw99/w44/wn//YhLX3JpxLmkQms7ur6n4i8jWAqq4MegglUtW3KJJAUFVHFLPteYnsMxafzK6gvv7akvjdeafVjFiwwJe2OZcCifQoNgbXOihsrkdREGqrSmH9evvyHkUFsmGDTU7vvz889ZRdGwEeJJxLkUQCxWPAq0BjEbkb+Bi4J9RWlULkqmwPFBXExx9Dp04wZAiccw7MmgUxVro555KnxKEnVX1RRL4EemFLXk9W1dmhtyxBnr6jAlmzBk46CXbYASZOtMpzzrmUKzFQiEhzYB3wevR9qrowzIYlyhMCVgAff2z5mbbfHt5805a/br99qlvlnAskMvT0JpZu/E3gfWA+MCHMRpWG9ygy2PLlNrx08MGFSfwOOMCDhHNpJpGhpw7Rt4PMsReF1qJS8h5FBlKFsWPh8svtF3j77dC/f6pb5ZwrRqlzPanqVyJS5gR+5c0DRQa6+moYOhS6dLG5iE6dUt0i51wcicxRXBN1cztgP2BpaC0qpZUrYbvtvORA2lOF/HzLx3TiibDrrnDNNZbUzzmX1hKZo6gb9VUDm6s4KcxGlUYkz9N2YWatctvmp5/g6KMLk/gdcQTccIMHCecyRNz/1OBCu+1V9foktafU/KrsNLZpEzzxBNxyC1SpAqedluoWOefKoNhAISJVVTU/0bKnqeJ5ntLU3Llw3nlWv7pPH7vCerfdSnyacy79xOtRTMPmI74RkdeAl4G1kQdVdVzIbUvIihXQoEGqW+G2kp8PP/8ML7wAZ5wBEqs8iXMuEyQySLwTsByrQqfY1dkKpEWgWLkSWrVKdSscANOnWxK/wYOhXTuYP9/zMzlXAcQLFI2DFU+5FAaIiKKV6lLGh57SwPr1MGgQPPQQ7Lyz1Yxo1MiDhHMVRLy1QlWA7YOvulE/R75SrqAA/vjDJ7NTavJk6NgRHngALrgAZs70JH7OVTDxehRLVPXfSWtJGfz5pwUL71GkyJo18I9/QP368P77tuzVOVfhxAsUaT/76Fdlp8iUKXDQQZaTacIEKypUp06qW+WcC0m8oadeSWtFGXlCwCRbtgzOOstqV0eS+HXr5kHCuQqu2B6Fqq5IZkPKwnsUSaIKL70EV1xh0XnQIE/i51wlktE5FCKBwnsUIbvySnj8cStN+v770KFDyc9xzlUYGR0ovAxqiFRh40aoXh369oXdd4errrJUHM65SiWjU+l5jyIkP/4IvXrBbbfZ7cMPh2uv9SDhXCWV0YFi5UqoVQtq1kx1SyqITZvg4YdtaOnLL6FNm1S3yDmXBjJ66Mmvyi5Hc+bAuefCtGlwwgnw5JPQtGmqW+WcSwMZHyh82KmcFBTA4sUwZgxkZXkSP+fcZhkdKFau9B7FNpk2zZL43X23JfH78UebvHbOuSgZPUfhQ09ltG4dXHcd9OgBzz0HS4PKth4knHMxZHSg8Op2ZfDhhzZZ/dBD8K9/eRI/51yJMnroyXsUpbRmjZUjrV/fAsZhh6W6Rc65DJCxPYq//rIRFA8UCZg0ySarI0n8ZszwIOGcS1jGBgpPCJiApUvh9NPtgrkXXrD79t8fatdObbuccxklY4eePCFgHKq2zHXgQFi92kqTehI/51wZZWyg8B5FHFdcAcOGwQEHwDPP2NJX55wro4wNFN6jKKKgAPLzbYnrqadCq1YWMDw/k3NuG4U6RyEivUXkexGZJyI3xXj8TBGZEXxNFZFOie7bA0WUH36wMqS33mq3DzvMM70658pNaIFCRKoAw4A+QDvgdBEpOgbyE3CoqnYEBgMjE92/Dz1hPYgHH4SOHeGbb6Bt21S3yDlXAYU59NQNmKeq8wFEJBs4CZgV2UBVp0Zt/xnQLNGdr1hh6Yjq1Sun1maa2bPhnHNg+nQ46SQYPhx23TXVrXLOVUBhDj01BRZF3c4L7ivOBcCEWA+IyAARmS4i05cG6SZWrrTrxrbL2AW+5eC33yAnB1591YOEcy40Yb7Nxko/qjE3FDkcCxQ3xnpcVUeqaldV7dooSDdRKa/K/uwzuPlm+7ltW0vi16+fZ3p1zoUqzECRB+wWdbsZsLjoRiLSERgFnKSqyxPdeaUKFGvXwtVXw4EHwosvFibxq1Ytte1yzlUKYQaKL4DWItJSRKoD/YHXojcQkebAOOBsVZ1bmp1XmoSA770H++wDjz4Kl17qSfycc0kX2mS2quaLyOXAO0AV4FlVnSkiFwePjwDuABoAw8WGT/JVtWsi+1+xAlq2DKftaWPNGruieqed4KOP4OCDU90i51wlFOoFd6r6FvBWkftGRP18IXBhWfZdoavbffABHHqoJfF75x27srpWrVS3yjlXSWXkmqGCggpa3e6332xyulevwiR+Xbp4kHDOpVRGBorVqy1YVJhAoQrPP289h0hp0jPOSHWrnHMOyNBcTxXuquzLLoMnn7TSpM8841dYO+fSSkYGigqR56mgADZuhBo1ICvLgsOll3p+Judc2snIoadIoMjYHsX339tkdSSJ36GHeqZX51zayshAERl6yrgexcaNMGQIdOoEubnQoUOqW+SccyXyoadkmTkTzj4bvv4a/vEPKyy0886pbpVzzpUoIwNFRk5mV6liEW7sWDjllFS3xjnnEpaRQ08rVkDNmhlwecHUqXBjkOdw771h3jwPEs65jJOxgSKtexNr1sDAgdCzp6UBX7bM7q+akR0451wll5GBIq2vyp440ZL4PfEEXH65TVo3bJjqVjnnXJll5EfctE0xvmYNnHkmNGgAU6bAQQelukXOObfNMrZHkVZDT+++C5s2WRK/iROtfrUHCedcBZGRgSJtehRLltjk9NFHW0EhgH33tZl255yrIDxQlIUqjB5tSfzefNMuovMkfs65Cirj5ihUrTJoSoeeLrkEnnrKVjWNGgVt2qSwMc6lr40bN5KXl8eGDRtS3ZRKo2bNmjRr1oxq5VgqOeMCRX6+fU96jyI6id8ZZ0DHjnDxxbBdRnbKnEuKvLw86tatS4sWLQiqWLoQqSrLly8nLy+PluVYAjTj3uU2bbLvSe1RzJ5tZUhvucVuH3KIZXr1IOFcXBs2bKBBgwYeJJJERGjQoEG59+Ay7p0uqT2KjRvhnnugc2eYM8cmqp1zpeJBIrnCON8ZN/QU6VGEHihmzoSzzrKlrqedBo8/Dk2ahHxQ55xLPxnbowh96KlqVVi1CsaNg5de8iDhXAZ79dVXERHmzJmz+b5JkyZx/PHHb7Hdeeedx9ixYwGbiL/pppto3bo1++yzD926dWPChAnb3JZ7772XVq1a0aZNG955552Y22RlZdG5c2c6d+5MixYt6Ny5MwDTpk3bfH+nTp149dVXt7k9ifAeRbQpU6xm9YMP2kqmuXM9P5NzFcCYMWPo2bMn2dnZ3HnnnQk95/bbb2fJkiXk5uZSo0YNfvvtNyZPnrxN7Zg1axbZ2dnMnDmTxYsXc+SRRzJ37lyqFClalpOTs/nna6+9lnr16gGwzz77MH36dKpWrcqSJUvo1KkTJ5xwAlVDfp/KuHfBSI8iOG/lY/VquOkmGD4cWra0nxs29CDhXDm66iobyS1PnTvDo4/G32bNmjV88sknfPjhh5x44okJBYp169bx9NNP89NPP1GjRg0AmjRpQr9+/bapvf/73//o378/NWrUoGXLlrRq1Ypp06bRo0ePmNurKi+99BIffPABALVr19782IYNG5I2/5NxQ0+bNkH9+uVYNXTCBGjfHp580v6Sv/vOk/g5V4GMHz+e3r17s9dee7HTTjvx1VdflficefPm0bx5c3bYYYcSt7366qs3DwdFfw0ZMmSrbX/55Rd22223zbebNWvGL7/8Uuy+p0yZQpMmTWjduvXm+z7//HPat29Phw4dGDFiROi9CcjQHkW5DTutXg3nnAONG1vtiAMOKKcdO+eKKumTf1jGjBnDVVddBUD//v0ZM2YM++23X7Gfxkv7Kf2RRx5JeFtVLdXxxowZw+mnn77Ffd27d2fmzJnMnj2bc889lz59+lAz5LRBGRkotmkiWxXeeQeOOgrq1oX33rOiQkH30jlXcSxfvpwPPviA3NxcRIRNmzYhItx///00aNCAlZFymYEVK1bQsGFDWrVqxcKFC1m9ejV169aNe4yrr76aDz/8cKv7+/fvz0033bTFfc2aNWPRokWbb+fl5bHrrrvG3G9+fj7jxo3jyy+/jPl427ZtqVOnDrm5uXTt2jVuG7eZqmbUV506XfSoo7RsFi9WPflkVVB97rky7sQ5l6hZs2al9PgjRozQAQMGbHHfIYccoh999JFu2LBBW7RosbmNCxYs0ObNm+sff/yhqqrXX3+9nnfeefrXX3+pqurixYv1+eef36b25ObmaseOHXXDhg06f/58bdmypebn58fcdsKECXrIIYdscd/8+fN148aNm9u7yy676NKlS7d6bqzzDkzXMr7vZtwcRZl6FKrw7LPQti28/Tbcf78n8XOuEhgzZgx9+/bd4r5TTjmF//73v9SoUYMXXniB888/n86dO3PqqacyatSozSuM7rrrLho1akS7du3YZ599OPnkk2nUqNE2tad9+/b069ePdu3a0bt3b4YNG7Z5xdOFF17I9OnTN2+bnZ291bDTxx9/TKdOnejcuTN9+/Zl+PDhNEzCnKpojDGzdFatWle98MLpPPlkKZ500UUwcqSl3hg1CqImhpxz4Zk9ezZt27ZNdTMqnVjnXUS+VNUyjVFl5BxFQpPZmzZZCo6aNe0K6333hQEDPD+Tc86VUka+a5Y49DRzplWYiyTxO/hgz/TqnHNllJHvnMX2KP7+GwYPtt7DvHmw//5JbZdzbmuZNryd6cI43xk39ATF9Ci++w7OPNO+9+8Pjz0G2zjx5JzbNjVr1mT58uWeajxJNKhHUd7XVWRkoIjZo6heHdats1xNJ56Y9DY557bWrFkz8vLyWLp0aaqbUmlEKtyVp8wOFJMnw2uvwUMPWRK/778vx9wezrltVa1atXKttOZSI9Q5ChHpLSLfi8g8EbkpxuMiIo8Fj88Qkf0S2e9OVf+0utWHHQbjx8OyZfaABwnnnCt3oQUKEakCDAP6AO2A00WkXZHN+gCtg68BQIlXR+zAKnY5qr1dF3HNNZ7EzznnQhbm0FM3YJ6qzgcQkWzgJGBW1DYnAf8JLi//TETqi8guqrqkuJ22ZAFSvw28Mha6dw+x+c455yDcQNEUWBR1Ow8o+s4ea5umwBaBQkQGYD0OgL+2mzkz1zO9AtAQWJbqRqQJPxeF/FwU8nNRqE1ZnxhmoIi1Fq7oAt9EtkFVRwIjAURkelkvQ69o/FwU8nNRyM9FIT8XhURkeslbxRbmZHYesFvU7WbA4jJs45xzLoXCDBRfAK1FpKWIVAf6A68V2eY14Jxg9dMBwKp48xPOOeeSL7ShJ1XNF5HLgXeAKsCzqjpTRC4OHh8BvAUcC8wD1gHnJ7DrkSE1ORP5uSjk56KQn4tCfi4KlflcZFyaceecc8mVkUkBnXPOJY8HCuecc3GlbaAIK/1HJkrgXJwZnIMZIjJVRDqlop3JUNK5iNpufxHZJCKnJrN9yZTIuRCRw0TkGxGZKSKTk93GZEngf6SeiLwuIt8G5yKR+dCMIyLPisjvIpJbzONle98sa7HtML+wye8fgT2A6sC3QLsi2xwLTMCuxTgA+DzV7U7huTgQ2DH4uU9lPhdR232ALZY4NdXtTuHfRX0sE0Lz4HbjVLc7hefiFuC+4OdGwAqgeqrbHsK5OATYD8gt5vEyvW+ma49ic/oPVf0biKT/iLY5/YeqfgbUF5Fdkt3QJCjxXKjqVFVdGdz8DLsepSJK5O8C4ArgFeD3ZDYuyRI5F2cA41R1IYCqVtTzkci5UKCuWFGM7bFAkZ/cZoZPVT/CXltxyvS+ma6BorjUHqXdpiIo7eu8APvEUBGVeC5EpCnQFxiRxHalQiJ/F3sBO4rIJBH5UkTOSVrrkiuRc/EE0Ba7oPc74EpVLUhO89JKmd4307UeRbml/6gAEn6dInI4Fih6htqi1EnkXDwK3Kiqmyp4RbVEzkVVoAvQC6gFfCoin6nq3LAbl2SJnItjgG+AI4A9gXdFZIqq/hly29JNmd430zVQePqPQgm9ThHpCIwC+qjq8iS1LdkSORddgewgSDQEjhWRfFUdn5QWJk+i/yPLVHUtsFZEPgI6ARUtUCRyLs4HhqgN1M8TkZ+AvYFpyWli2ijT+2a6Dj15+o9CJZ4LEWkOjAPOroCfFqOVeC5UtaWqtlDVFsBY4NIKGCQgsf+R/wEHi0hVEamNZW+eneR2JkMi52Ih1rNCRJpgmVTnJ7WV6aFM75tp2aPQ8NJ/ZJwEz8UdQANgePBJOl8rYMbMBM9FpZDIuVDV2SLyNjADKABGqWrMZZOZLMG/i8HAaBH5Dht+uVFVK1z6cREZAxwGNBSRPGAQUA227X3TU3g455yLK12HnpxzzqUJDxTOOefi8kDhnHMuLg8Uzjnn4vJA4ZxzLi4PFC4tBZlfv4n6ahFn2zXlcLzRIvJTcKyvRKRHGfYxSkTaBT/fUuSxqdvaxmA/kfOSG2RDrV/C9p1F5NjyOLarvHx5rEtLIrJGVbcv723j7GM08IaqjhWRo4EHVbXjNuxvm9tU0n5F5DlgrqreHWf784Cuqnp5ebfFVR7eo3AZQUS2F5H3g0/734nIVlljRWQXEfko6hP3wcH9R4vIp8FzXxaRkt7APwJaBc+9JthXrohcFdxXR0TeDGob5IpIVnD/JBHpKiJDgFpBO14MHlsTfM+J/oQf9GROEZEqIvKAiHwhVifgogROy6cECd1EpJtYLZKvg+9tgquU/w1kBW3JCtr+bHCcr2OdR+e2kur86f7lX7G+gE1YErdvgFexLAI7BI81xK4sjfSI1wTfrwVuDX6uAtQNtv0IqBPcfyNwR4zjjSaoXQGcBnyOJdT7DqiDpaaeCewLnAI8HfXcesH3Sdin981titom0sa+wHPBz9WxTJ61gAHAbcH9NYDpQMsY7VwT9fpeBnoHt3cAqgY/Hwm8Evx8HvBE1PPvAc4Kfq6P5X2qk+rft3+l91dapvBwDlivqp0jN0SkGnCPiByCpaNoCjQBfo16zhfAs8G241X1GxE5FGgHfBKkN6mOfRKP5QERuQ1YimXh7QW8qpZUDxEZBxwMvA08KCL3YcNVU0rxuiYAj4lIDaA38JGqrg+GuzpKYUW+ekBr4Kciz68lIt8ALYAvgXejtn9ORFpj2UCrFXP8o4ETReS64HZNoDkVMweUKyceKFymOBOrTNZFVTeKyALsTW4zVf0oCCTHAc+LyAPASuBdVT09gWNcr6pjIzdE5MhYG6nqXBHpguXMuVdEJqrqvxN5Eaq6QUQmYWmvs4AxkcMBV6jqOyXsYr2qdhaResAbwGXAY1guow9VtW8w8T+pmOcLcIqqfp9Ie50Dn6NwmaMe8HsQJA4Hdi+6gYjsHmzzNPAMVhLyM+AgEYnMOdQWkb0SPOZHwMnBc+pgw0ZTRGRXYJ2qvgA8GBynqI1BzyaWbCwZ28FYIjuC75dEniMiewXHjElVVwEDgeuC59QDfgkePi9q09XYEFzEO8AVEnSvRGTf4o7hXIQHCpcpXgS6ish0rHcxJ8Y2hwHfiMjX2DzCUFVdir1xjhGRGVjg2DuRA6rqV9jcxTRszmKUqn4NdACmBUNAtwJ3xXj6SGBGZDK7iIlYbeP31Ep3gtUSmQV8JSK5wFOU0OMP2vItllb7fqx38wk2fxHxIdAuMpmN9TyqBW3LDW47F5cvj3XOOReX9yicc87F5YHCOedcXB4onHPOxeWBwjnnXFweKJxzzsXlgcI551xcHiicc87F9f8BCD//09vjrzYAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sklearn.metrics as metrics\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "fpr, tpr, threshold = metrics.roc_curve(kerasYDev, predictions)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# method I: plt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic (ROC)')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.949200376293509\n",
      "0.6185567010309279\n",
      "0.4580152671755725\n"
     ]
    }
   ],
   "source": [
    "predictionsTest = []\n",
    "for i in y_test:\n",
    "    if i[0] > .75:\n",
    "        predictionsTest.append(0)\n",
    "    else:\n",
    "        predictionsTest.append(1)\n",
    "\n",
    "accuracy = accuracy_score(kerasYTest, predictionsTest)\n",
    "precision = precision_score(kerasYTest, predictionsTest)\n",
    "recall = recall_score(kerasYTest, predictionsTest)\n",
    "\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PCA does not support sparse input. See TruncatedSVD for a possible alternative.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-313-b33dcfab7a1f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mpca\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_components\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mtest_data_keras_PCA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpca\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXpred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mclass1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mclass2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    374\u001b[0m         \u001b[0mC\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mordered\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse\u001b[0m \u001b[0;34m'np.ascontiguousarray'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    375\u001b[0m         \"\"\"\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0mU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mV\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m         \u001b[0mU\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mU\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_components_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/sklearn/decomposition/_pca.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;31m# This is more informative than the generic one raised by check_array.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    393\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 394\u001b[0;31m             raise TypeError('PCA does not support sparse input. See '\n\u001b[0m\u001b[1;32m    395\u001b[0m                             'TruncatedSVD for a possible alternative.')\n\u001b[1;32m    396\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: PCA does not support sparse input. See TruncatedSVD for a possible alternative."
     ]
    }
   ],
   "source": [
    "# Predicted landscape of our data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "test_data_keras_PCA = pca.fit_transform(Xpred)\n",
    "class1 = []\n",
    "class2 = []\n",
    "\n",
    "for a in range(len(Xpred)):\n",
    "    if predictions[a] == 1:\n",
    "        class1.append(test_data_keras_PCA[a])\n",
    "    else:\n",
    "        class2.append(test_data_keras_PCA[a])\n",
    "                         \n",
    "plt.scatter(*zip(*class2), color='blue')\n",
    "plt.scatter(*zip(*class1), color='red')\n",
    "# plt.xlim(-.4, 1.4)\n",
    "# plt.ylim(-.6, .6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "clf = MLPClassifier(hidden_layer_sizes=(100, ), random_state=None, max_iter=1000, verbose=True).fit(XBigram, kerasY)\n",
    "y_pred = clf.predict(XPredBigram)\n",
    "accuracy(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(kerasYDev, y_pred)\n",
    "precision = precision_score(kerasYDev, y_pred)\n",
    "recall = recall_score(kerasYDev, y_pred)\n",
    "\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "169\n"
     ]
    }
   ],
   "source": [
    "homelessWords = [\n",
    "    \"undomicile\",\n",
    "    \"homeless\",\n",
    "    \"unhoused\",\n",
    "]\n",
    "\n",
    "y_pred=[]\n",
    "count = 0\n",
    "\n",
    "for index, row in notesAndLabels2.iterrows():\n",
    "    homeless = False\n",
    "    for i in homelessWords:\n",
    "        if i in row['noteText']:\n",
    "            homeless = True\n",
    "    if homeless:\n",
    "        y_pred.append(1)\n",
    "        count += 1\n",
    "    else:\n",
    "        y_pred.append(0)\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9280037400654512\n",
      "0.3905325443786982\n",
      "0.5641025641025641\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(kerasYDev, y_pred)\n",
    "precision = precision_score(kerasYDev, y_pred)\n",
    "recall = recall_score(kerasYDev, y_pred)\n",
    "\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "InvalidArgumentError",
     "evalue": "indices[1] = [0,2061547] is out of order. Many sparse ops require sorted indices.\n    Use `tf.sparse.reorder` to create a correctly ordered copy.\n\n [Op:SerializeManySparse]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-47-77e7c5a4de70>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0madam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXBigram\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkerasY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m400\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1047\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1048\u001b[0m       \u001b[0;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1049\u001b[0;31m       data_handler = data_adapter.DataHandler(\n\u001b[0m\u001b[1;32m   1050\u001b[0m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution)\u001b[0m\n\u001b[1;32m   1103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1105\u001b[0;31m     self._adapter = adapter_cls(\n\u001b[0m\u001b[1;32m   1106\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1107\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    571\u001b[0m     \u001b[0minputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpack_x_y_sample_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 573\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdataset_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDatasetV2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_tensor_slices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    574\u001b[0m     \u001b[0mnum_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    575\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mfrom_tensor_slices\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    680\u001b[0m       \u001b[0mDataset\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mA\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m     \"\"\"\n\u001b[0;32m--> 682\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mTensorSliceDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    683\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    684\u001b[0m   \u001b[0;32mclass\u001b[0m \u001b[0m_GeneratorState\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, element)\u001b[0m\n\u001b[1;32m   3001\u001b[0m     \u001b[0melement\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize_element\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3002\u001b[0m     \u001b[0mbatched_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype_spec_from_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3003\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructure\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_batched_tensor_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatched_spec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0melement\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3004\u001b[0m     self._structure = nest.map_structure(\n\u001b[1;32m   3005\u001b[0m         lambda component_spec: component_spec._unbatch(), batched_spec)  # pylint: disable=protected-access\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mto_batched_tensor_list\u001b[0;34m(element_spec, element)\u001b[0m\n\u001b[1;32m    348\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m   \u001b[0;31m# pylint: disable=g-long-lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m   return _to_tensor_list_helper(\n\u001b[0m\u001b[1;32m    351\u001b[0m       lambda state, spec, component: state + spec._to_batched_tensor_list(\n\u001b[1;32m    352\u001b[0m           component), element_spec, element)\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m_to_tensor_list_helper\u001b[0;34m(encode_fn, element_spec, element)\u001b[0m\n\u001b[1;32m    323\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mencode_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 325\u001b[0;31m   return functools.reduce(\n\u001b[0m\u001b[1;32m    326\u001b[0m       reduce_fn, zip(nest.flatten(element_spec), nest.flatten(element)), [])\n\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36mreduce_fn\u001b[0;34m(state, value)\u001b[0m\n\u001b[1;32m    321\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 323\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mencode_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcomponent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m   return functools.reduce(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/data/util/structure.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(state, spec, component)\u001b[0m\n\u001b[1;32m    349\u001b[0m   \u001b[0;31m# pylint: disable=g-long-lambda\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m   return _to_tensor_list_helper(\n\u001b[0;32m--> 351\u001b[0;31m       lambda state, spec, component: state + spec._to_batched_tensor_list(\n\u001b[0m\u001b[1;32m    352\u001b[0m           component), element_spec, element)\n\u001b[1;32m    353\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/sparse_tensor.py\u001b[0m in \u001b[0;36m_to_batched_tensor_list\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m    342\u001b[0m       raise ValueError(\n\u001b[1;32m    343\u001b[0m           \"Unbatching a sparse tensor is only supported for rank >= 1\")\n\u001b[0;32m--> 344\u001b[0;31m     return [gen_sparse_ops.serialize_many_sparse(\n\u001b[0m\u001b[1;32m    345\u001b[0m         \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m         out_type=dtypes.variant)]\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/gen_sparse_ops.py\u001b[0m in \u001b[0;36mserialize_many_sparse\u001b[0;34m(sparse_indices, sparse_values, sparse_shape, out_type, name)\u001b[0m\n\u001b[1;32m    496\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    499\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6841\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6842\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6843\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6844\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6845\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: indices[1] = [0,2061547] is out of order. Many sparse ops require sorted indices.\n    Use `tf.sparse.reorder` to create a correctly ordered copy.\n\n [Op:SerializeManySparse]"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.01, momentum=0.0, nesterov=False, name=\"SGD\")\n",
    "adam = keras.optimizers.Adam(lr=0.00001, beta_1=0.9, beta_2=0.999, epsilon=1e-10, decay=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(128, input_dim=200, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(16, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='mean_squared_error', optimizer=adam)\n",
    "\n",
    "model.fit(kerasX, kerasY, epochs=200, batch_size=400, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n",
      "2139\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "cols1=['index', 'mrn', 'vector']\n",
    "cols2=['mrn', 'label']\n",
    "\n",
    "notesW2VG = pd.read_csv('notesVectorized.csv', dtype=object, header=None, names=cols1)\n",
    "labelsW2VG = pd.read_csv('train_data.csv', header=None, names=cols2)\n",
    "\n",
    "notesW2VG['mrn'].replace('', np.nan, inplace=True)\n",
    "notesW2VG.dropna(subset=['mrn'], inplace=True)\n",
    "\n",
    "notesW2VG['mrn'] = notesW2VG['mrn'].astype(int)\n",
    "labelsW2VG['mrn'] = labelsW2VG['mrn'].astype(int)\n",
    "\n",
    "notesAndLabelsW2VG = pd.merge(notesW2VG, labelsW2VG, how='left', on=['mrn'])\n",
    "\n",
    "kerasXW2VG = []\n",
    "kerasYW2VG = []\n",
    "for index, row in notesAndLabelsW2VG.iterrows():\n",
    "    vectorRow = []\n",
    "    row2 = \"\"\n",
    "    row2 = row['vector'].replace(\"[\", '').replace(\"]\", '')\n",
    "    row2 = re.split('\\\\s+', row2)\n",
    "    for i in row2:\n",
    "        if i != \"\":\n",
    "            vectorRow.append(float(i))\n",
    "    kerasXW2VG.append(vectorRow)\n",
    "    kerasYW2VG.append(row['label'])\n",
    "\n",
    "print(len(kerasXW2VG[0]))\n",
    "\n",
    "cols1=['index', 'mrn', 'vector']\n",
    "cols2=['mrn', 'label']\n",
    "\n",
    "notes2W2VG = pd.read_csv('dev_notesVectorized.csv', dtype=object, header=None, names=cols1)\n",
    "labels2W2VG = pd.read_csv('validate_data.csv', header=None, names=cols2)\n",
    "\n",
    "notes2W2VG['mrn'].replace('', np.nan, inplace=True)\n",
    "notes2W2VG.dropna(subset=['mrn'], inplace=True)\n",
    "\n",
    "notes2W2VG['mrn'] = notes2W2VG['mrn'].astype(int)\n",
    "labels2W2VG['mrn'] = labels2W2VG['mrn'].astype(int)\n",
    "\n",
    "notesAndLabels2W2VG = pd.merge(notes2W2VG, labels2W2V, how='left', on=['mrn'])\n",
    "\n",
    "kerasXDevW2VG = []\n",
    "kerasYDevW2VG =[]\n",
    "for index, row in notesAndLabels2W2VG.iterrows():\n",
    "    vectorRow = []\n",
    "    row2 = \"\"\n",
    "    row2 = row['vector'].replace(\"[\", '').replace(\"]\", '')\n",
    "    row2 = re.split('\\\\s+', row2)\n",
    "    for i in row2:\n",
    "        if i != \"\":\n",
    "            vectorRow.append(float(i))\n",
    "    kerasXDevW2VG.append(vectorRow)\n",
    "    kerasYDevW2VG.append(row['label'])\n",
    "print(len(kerasYDevW2VG))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.1197\n",
      "Epoch 2/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0591\n",
      "Epoch 3/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 4/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 5/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 6/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 7/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 8/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 9/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 10/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 11/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 12/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0588\n",
      "Epoch 13/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0588\n",
      "Epoch 14/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 15/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 16/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 17/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 18/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 19/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0588\n",
      "Epoch 20/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 21/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 22/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 23/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 24/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 25/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 26/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 27/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 28/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 29/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 30/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 31/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 32/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 33/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 34/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 35/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 36/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 37/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 38/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 39/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0588\n",
      "Epoch 40/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 41/1000\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0588\n",
      "Epoch 42/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 43/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 44/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0588\n",
      "Epoch 45/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0588\n",
      "Epoch 46/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 47/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0588\n",
      "Epoch 48/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 49/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 50/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 51/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 52/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 53/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 54/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 55/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 56/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 57/1000\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.0588\n",
      "Epoch 58/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 59/1000\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0588\n",
      "Epoch 60/1000\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0587\n",
      "Epoch 61/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0587\n",
      "Epoch 62/1000\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.0588\n",
      "Epoch 63/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0587\n",
      "Epoch 64/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 65/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 66/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0588\n",
      "Epoch 67/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0587\n",
      "Epoch 68/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0587\n",
      "Epoch 69/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0588\n",
      "Epoch 70/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0588\n",
      "Epoch 71/1000\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0588\n",
      "Epoch 72/1000\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0587\n",
      "Epoch 73/1000\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0587\n",
      "Epoch 74/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0587\n",
      "Epoch 75/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0587\n",
      "Epoch 76/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0587\n",
      "Epoch 77/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0587\n",
      "Epoch 78/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0587\n",
      "Epoch 79/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0587\n",
      "Epoch 80/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0586\n",
      "Epoch 81/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0586\n",
      "Epoch 82/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0585\n",
      "Epoch 83/1000\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0583\n",
      "Epoch 84/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0582\n",
      "Epoch 85/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0578\n",
      "Epoch 86/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0572\n",
      "Epoch 87/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0564\n",
      "Epoch 88/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0556\n",
      "Epoch 89/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0551\n",
      "Epoch 90/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0549\n",
      "Epoch 91/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0544\n",
      "Epoch 92/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0542\n",
      "Epoch 93/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0540\n",
      "Epoch 94/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0533\n",
      "Epoch 95/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0531\n",
      "Epoch 96/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0531\n",
      "Epoch 97/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0528\n",
      "Epoch 98/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0525\n",
      "Epoch 99/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0525\n",
      "Epoch 100/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0518\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 101/1000\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0516\n",
      "Epoch 102/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0514\n",
      "Epoch 103/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0511\n",
      "Epoch 104/1000\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0503\n",
      "Epoch 105/1000\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0510\n",
      "Epoch 106/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0499\n",
      "Epoch 107/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0500\n",
      "Epoch 108/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0496\n",
      "Epoch 109/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0492\n",
      "Epoch 110/1000\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0489\n",
      "Epoch 111/1000\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0487\n",
      "Epoch 112/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0483\n",
      "Epoch 113/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0488\n",
      "Epoch 114/1000\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0491\n",
      "Epoch 115/1000\n",
      "22/22 [==============================] - 0s 4ms/step - loss: 0.0484\n",
      "Epoch 116/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0482\n",
      "Epoch 117/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0487\n",
      "Epoch 118/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0480\n",
      "Epoch 119/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0477\n",
      "Epoch 120/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0476\n",
      "Epoch 121/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0473\n",
      "Epoch 122/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0470\n",
      "Epoch 123/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0472\n",
      "Epoch 124/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0491\n",
      "Epoch 125/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0486\n",
      "Epoch 126/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0474\n",
      "Epoch 127/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0488\n",
      "Epoch 128/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0470\n",
      "Epoch 129/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0471\n",
      "Epoch 130/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0476\n",
      "Epoch 131/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0471\n",
      "Epoch 132/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0480\n",
      "Epoch 133/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0468\n",
      "Epoch 134/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0474\n",
      "Epoch 135/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0480\n",
      "Epoch 136/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0466\n",
      "Epoch 137/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0466\n",
      "Epoch 138/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0466\n",
      "Epoch 139/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0464\n",
      "Epoch 140/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0472\n",
      "Epoch 141/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0477\n",
      "Epoch 142/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0463\n",
      "Epoch 143/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0469\n",
      "Epoch 144/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0467\n",
      "Epoch 145/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0464\n",
      "Epoch 146/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0461\n",
      "Epoch 147/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0460\n",
      "Epoch 148/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0465\n",
      "Epoch 149/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0466\n",
      "Epoch 150/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0464\n",
      "Epoch 151/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0459\n",
      "Epoch 152/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0465\n",
      "Epoch 153/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0461\n",
      "Epoch 154/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0459\n",
      "Epoch 155/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0463\n",
      "Epoch 156/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0464\n",
      "Epoch 157/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0460\n",
      "Epoch 158/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0459\n",
      "Epoch 159/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0457\n",
      "Epoch 160/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0461\n",
      "Epoch 161/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0457\n",
      "Epoch 162/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0458\n",
      "Epoch 163/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0457\n",
      "Epoch 164/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0458\n",
      "Epoch 165/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0458\n",
      "Epoch 166/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0459\n",
      "Epoch 167/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0457\n",
      "Epoch 168/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0456\n",
      "Epoch 169/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0473\n",
      "Epoch 170/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0468\n",
      "Epoch 171/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0457\n",
      "Epoch 172/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0472\n",
      "Epoch 173/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0457\n",
      "Epoch 174/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0459\n",
      "Epoch 175/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0458\n",
      "Epoch 176/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0456\n",
      "Epoch 177/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0455\n",
      "Epoch 178/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0458\n",
      "Epoch 179/1000\n",
      "22/22 [==============================] - 0s 13ms/step - loss: 0.0454\n",
      "Epoch 180/1000\n",
      "22/22 [==============================] - 0s 17ms/step - loss: 0.0457\n",
      "Epoch 181/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0455\n",
      "Epoch 182/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0462\n",
      "Epoch 183/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0459\n",
      "Epoch 184/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0470\n",
      "Epoch 185/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0452\n",
      "Epoch 186/1000\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.0462\n",
      "Epoch 187/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0456\n",
      "Epoch 188/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0453\n",
      "Epoch 189/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0458\n",
      "Epoch 190/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0452\n",
      "Epoch 191/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0452\n",
      "Epoch 192/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0454\n",
      "Epoch 193/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0454\n",
      "Epoch 194/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0456\n",
      "Epoch 195/1000\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.0462\n",
      "Epoch 196/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0455\n",
      "Epoch 197/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0454\n",
      "Epoch 198/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0459\n",
      "Epoch 199/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0449\n",
      "Epoch 200/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0450\n",
      "Epoch 201/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0449\n",
      "Epoch 202/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0449\n",
      "Epoch 203/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0449\n",
      "Epoch 204/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0450A: 0s - loss: 0.04\n",
      "Epoch 205/1000\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.0452\n",
      "Epoch 206/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0449\n",
      "Epoch 207/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0451\n",
      "Epoch 208/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0450\n",
      "Epoch 209/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0451\n",
      "Epoch 210/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0447\n",
      "Epoch 211/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0451\n",
      "Epoch 212/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0460\n",
      "Epoch 213/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0449\n",
      "Epoch 214/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0450\n",
      "Epoch 215/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0448\n",
      "Epoch 216/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0446\n",
      "Epoch 217/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0447\n",
      "Epoch 218/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0452\n",
      "Epoch 219/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0447\n",
      "Epoch 220/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0449\n",
      "Epoch 221/1000\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.0453A: 0s - loss: 0.0\n",
      "Epoch 222/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0457\n",
      "Epoch 223/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0452\n",
      "Epoch 224/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0451\n",
      "Epoch 225/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0448\n",
      "Epoch 226/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0451\n",
      "Epoch 227/1000\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.0452\n",
      "Epoch 228/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0444\n",
      "Epoch 229/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0453\n",
      "Epoch 230/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0446\n",
      "Epoch 231/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0450\n",
      "Epoch 232/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0459\n",
      "Epoch 233/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0452\n",
      "Epoch 234/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0456\n",
      "Epoch 235/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0444\n",
      "Epoch 236/1000\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.0450\n",
      "Epoch 237/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0446\n",
      "Epoch 238/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0448\n",
      "Epoch 239/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0452\n",
      "Epoch 240/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0460\n",
      "Epoch 241/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0445\n",
      "Epoch 242/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0445\n",
      "Epoch 243/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0445\n",
      "Epoch 244/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0447\n",
      "Epoch 245/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0445\n",
      "Epoch 246/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0454\n",
      "Epoch 247/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0460\n",
      "Epoch 248/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0449\n",
      "Epoch 249/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0447\n",
      "Epoch 250/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0444\n",
      "Epoch 251/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0444\n",
      "Epoch 252/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0445\n",
      "Epoch 253/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0444\n",
      "Epoch 254/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0448\n",
      "Epoch 255/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0441\n",
      "Epoch 256/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0445\n",
      "Epoch 257/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0440\n",
      "Epoch 258/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0447\n",
      "Epoch 259/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0441\n",
      "Epoch 260/1000\n",
      "22/22 [==============================] - 0s 16ms/step - loss: 0.0448\n",
      "Epoch 261/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0444\n",
      "Epoch 262/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0441\n",
      "Epoch 263/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0439\n",
      "Epoch 264/1000\n",
      "22/22 [==============================] - 0s 11ms/step - loss: 0.0447\n",
      "Epoch 265/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0442\n",
      "Epoch 266/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0441\n",
      "Epoch 267/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0443\n",
      "Epoch 268/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0439\n",
      "Epoch 269/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0439\n",
      "Epoch 270/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0442\n",
      "Epoch 271/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0439\n",
      "Epoch 272/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0442\n",
      "Epoch 273/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0446\n",
      "Epoch 274/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0455\n",
      "Epoch 275/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0444\n",
      "Epoch 276/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0437\n",
      "Epoch 277/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0441\n",
      "Epoch 278/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0447\n",
      "Epoch 279/1000\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.0443\n",
      "Epoch 280/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0441\n",
      "Epoch 281/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0438\n",
      "Epoch 282/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0436\n",
      "Epoch 283/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0442\n",
      "Epoch 284/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0438\n",
      "Epoch 285/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0439\n",
      "Epoch 286/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0438\n",
      "Epoch 287/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0436\n",
      "Epoch 288/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0439\n",
      "Epoch 289/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0444\n",
      "Epoch 290/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0440\n",
      "Epoch 291/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0436\n",
      "Epoch 292/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0437\n",
      "Epoch 293/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0438\n",
      "Epoch 294/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0442\n",
      "Epoch 295/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0442\n",
      "Epoch 296/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0437\n",
      "Epoch 297/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0441\n",
      "Epoch 298/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0436\n",
      "Epoch 299/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0434\n",
      "Epoch 300/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0436\n",
      "Epoch 301/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0435\n",
      "Epoch 302/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0433\n",
      "Epoch 303/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0443\n",
      "Epoch 304/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0436\n",
      "Epoch 305/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0438\n",
      "Epoch 306/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0436\n",
      "Epoch 307/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0436\n",
      "Epoch 308/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0433\n",
      "Epoch 309/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0434\n",
      "Epoch 310/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0435\n",
      "Epoch 311/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0437\n",
      "Epoch 312/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0433\n",
      "Epoch 313/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0434\n",
      "Epoch 314/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0434\n",
      "Epoch 315/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0443\n",
      "Epoch 316/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0435\n",
      "Epoch 317/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0435\n",
      "Epoch 318/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0433\n",
      "Epoch 319/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0432\n",
      "Epoch 320/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0434\n",
      "Epoch 321/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0436\n",
      "Epoch 322/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0432\n",
      "Epoch 323/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0431\n",
      "Epoch 324/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0430\n",
      "Epoch 325/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0432\n",
      "Epoch 326/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0430\n",
      "Epoch 327/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0437\n",
      "Epoch 328/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0437\n",
      "Epoch 329/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0440\n",
      "Epoch 330/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0438\n",
      "Epoch 331/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0434\n",
      "Epoch 332/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0429\n",
      "Epoch 333/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0430\n",
      "Epoch 334/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0435\n",
      "Epoch 335/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0436\n",
      "Epoch 336/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0429\n",
      "Epoch 337/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0429\n",
      "Epoch 338/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0438\n",
      "Epoch 339/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0435\n",
      "Epoch 340/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0438\n",
      "Epoch 341/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0428\n",
      "Epoch 342/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0427\n",
      "Epoch 343/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0435\n",
      "Epoch 344/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0428\n",
      "Epoch 345/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0429\n",
      "Epoch 346/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0430\n",
      "Epoch 347/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0428\n",
      "Epoch 348/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0429\n",
      "Epoch 349/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0429\n",
      "Epoch 350/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0431\n",
      "Epoch 351/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0430\n",
      "Epoch 352/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0432\n",
      "Epoch 353/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0426\n",
      "Epoch 354/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0431\n",
      "Epoch 355/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0427\n",
      "Epoch 356/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0428\n",
      "Epoch 357/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0432\n",
      "Epoch 358/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0426\n",
      "Epoch 359/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0434\n",
      "Epoch 360/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0429\n",
      "Epoch 361/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0437\n",
      "Epoch 362/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0434\n",
      "Epoch 363/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0440\n",
      "Epoch 364/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0426\n",
      "Epoch 365/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0429\n",
      "Epoch 366/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0438\n",
      "Epoch 367/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0430\n",
      "Epoch 368/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0427\n",
      "Epoch 369/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0426\n",
      "Epoch 370/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0426\n",
      "Epoch 371/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0429\n",
      "Epoch 372/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0431\n",
      "Epoch 373/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0439\n",
      "Epoch 374/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0426\n",
      "Epoch 375/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0424\n",
      "Epoch 376/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0426\n",
      "Epoch 377/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0427\n",
      "Epoch 378/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0425\n",
      "Epoch 379/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0423\n",
      "Epoch 380/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0426\n",
      "Epoch 381/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0426\n",
      "Epoch 382/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0428\n",
      "Epoch 383/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0427\n",
      "Epoch 384/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0435\n",
      "Epoch 385/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0428\n",
      "Epoch 386/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0423\n",
      "Epoch 387/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0430\n",
      "Epoch 388/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0425\n",
      "Epoch 389/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0431\n",
      "Epoch 390/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0438\n",
      "Epoch 391/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0425\n",
      "Epoch 392/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0423\n",
      "Epoch 393/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0431\n",
      "Epoch 394/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0426\n",
      "Epoch 395/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0437\n",
      "Epoch 396/1000\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.0429\n",
      "Epoch 397/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0423\n",
      "Epoch 398/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0425\n",
      "Epoch 399/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0424\n",
      "Epoch 400/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0429\n",
      "Epoch 401/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0431\n",
      "Epoch 402/1000\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.0425\n",
      "Epoch 403/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0427\n",
      "Epoch 404/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0421\n",
      "Epoch 405/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0422\n",
      "Epoch 406/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0420\n",
      "Epoch 407/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0445\n",
      "Epoch 408/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0433\n",
      "Epoch 409/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0421\n",
      "Epoch 410/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0419A: 0s - loss: 0.04\n",
      "Epoch 411/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0422\n",
      "Epoch 412/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0424\n",
      "Epoch 413/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0435\n",
      "Epoch 414/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0430\n",
      "Epoch 415/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0420\n",
      "Epoch 416/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0428\n",
      "Epoch 417/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0430\n",
      "Epoch 418/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0421\n",
      "Epoch 419/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0420\n",
      "Epoch 420/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0424\n",
      "Epoch 421/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0419\n",
      "Epoch 422/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0426\n",
      "Epoch 423/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0419\n",
      "Epoch 424/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0419\n",
      "Epoch 425/1000\n",
      "22/22 [==============================] - 0s 10ms/step - loss: 0.0428\n",
      "Epoch 426/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0429\n",
      "Epoch 427/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0422\n",
      "Epoch 428/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0421\n",
      "Epoch 429/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0419\n",
      "Epoch 430/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0418\n",
      "Epoch 431/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0433\n",
      "Epoch 432/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0423\n",
      "Epoch 433/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0418\n",
      "Epoch 434/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0430\n",
      "Epoch 435/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0418\n",
      "Epoch 436/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0418\n",
      "Epoch 437/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0417\n",
      "Epoch 438/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0420\n",
      "Epoch 439/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0419\n",
      "Epoch 440/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0421\n",
      "Epoch 441/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0418\n",
      "Epoch 442/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0421\n",
      "Epoch 443/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0425\n",
      "Epoch 444/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0420\n",
      "Epoch 445/1000\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.0418\n",
      "Epoch 446/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0425\n",
      "Epoch 447/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0420\n",
      "Epoch 448/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0415\n",
      "Epoch 449/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0420\n",
      "Epoch 450/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0422\n",
      "Epoch 451/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0420\n",
      "Epoch 452/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0417\n",
      "Epoch 453/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0415\n",
      "Epoch 454/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0426\n",
      "Epoch 455/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0420\n",
      "Epoch 456/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0417\n",
      "Epoch 457/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0416\n",
      "Epoch 458/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0415\n",
      "Epoch 459/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0413\n",
      "Epoch 460/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0419\n",
      "Epoch 461/1000\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.0416\n",
      "Epoch 462/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0417\n",
      "Epoch 463/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0416\n",
      "Epoch 464/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0419\n",
      "Epoch 465/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0414\n",
      "Epoch 466/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0415\n",
      "Epoch 467/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0414\n",
      "Epoch 468/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0412\n",
      "Epoch 469/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0415\n",
      "Epoch 470/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0414\n",
      "Epoch 471/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0412\n",
      "Epoch 472/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0419\n",
      "Epoch 473/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0413\n",
      "Epoch 474/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0413\n",
      "Epoch 475/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0418\n",
      "Epoch 476/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0428\n",
      "Epoch 477/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0421\n",
      "Epoch 478/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0417\n",
      "Epoch 479/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0419\n",
      "Epoch 480/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0413\n",
      "Epoch 481/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0421\n",
      "Epoch 482/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0410\n",
      "Epoch 483/1000\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.0430A: 0s - loss: 0.043\n",
      "Epoch 484/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0414\n",
      "Epoch 485/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0411\n",
      "Epoch 486/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0420\n",
      "Epoch 487/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0412\n",
      "Epoch 488/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0409\n",
      "Epoch 489/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0434\n",
      "Epoch 490/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0411\n",
      "Epoch 491/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0423\n",
      "Epoch 492/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0421\n",
      "Epoch 493/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0412\n",
      "Epoch 494/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0414\n",
      "Epoch 495/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0416\n",
      "Epoch 496/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0414\n",
      "Epoch 497/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0410\n",
      "Epoch 498/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0411\n",
      "Epoch 499/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0412\n",
      "Epoch 500/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0413\n",
      "Epoch 501/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0411\n",
      "Epoch 502/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0416\n",
      "Epoch 503/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0433\n",
      "Epoch 504/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0419\n",
      "Epoch 505/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0408\n",
      "Epoch 506/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0415\n",
      "Epoch 507/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0411\n",
      "Epoch 508/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0415\n",
      "Epoch 509/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0424\n",
      "Epoch 510/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0416\n",
      "Epoch 511/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0413\n",
      "Epoch 512/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0411\n",
      "Epoch 513/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0410\n",
      "Epoch 514/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0414\n",
      "Epoch 515/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0406\n",
      "Epoch 516/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0413\n",
      "Epoch 517/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0419\n",
      "Epoch 518/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0415\n",
      "Epoch 519/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0409\n",
      "Epoch 520/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0411\n",
      "Epoch 521/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0412\n",
      "Epoch 522/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0409\n",
      "Epoch 523/1000\n",
      "22/22 [==============================] - 0s 9ms/step - loss: 0.0408\n",
      "Epoch 524/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0418\n",
      "Epoch 525/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0409\n",
      "Epoch 526/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0410\n",
      "Epoch 527/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0410\n",
      "Epoch 528/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0412\n",
      "Epoch 529/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0415\n",
      "Epoch 530/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0421\n",
      "Epoch 531/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0422\n",
      "Epoch 532/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0426\n",
      "Epoch 533/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0414\n",
      "Epoch 534/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0410\n",
      "Epoch 535/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0408\n",
      "Epoch 536/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0410\n",
      "Epoch 537/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0415\n",
      "Epoch 538/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0411\n",
      "Epoch 539/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0426\n",
      "Epoch 540/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0425\n",
      "Epoch 541/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0414\n",
      "Epoch 542/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0420\n",
      "Epoch 543/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0414\n",
      "Epoch 544/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0409\n",
      "Epoch 545/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0406\n",
      "Epoch 546/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0409\n",
      "Epoch 547/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0405\n",
      "Epoch 548/1000\n",
      "22/22 [==============================] - 0s 5ms/step - loss: 0.0414\n",
      "Epoch 549/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0406\n",
      "Epoch 550/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0409\n",
      "Epoch 551/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0405\n",
      "Epoch 552/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0405\n",
      "Epoch 553/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0407\n",
      "Epoch 554/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0405\n",
      "Epoch 555/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0408\n",
      "Epoch 556/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0410\n",
      "Epoch 557/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0410\n",
      "Epoch 558/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0420\n",
      "Epoch 559/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0419\n",
      "Epoch 560/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0411\n",
      "Epoch 561/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0426\n",
      "Epoch 562/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0413\n",
      "Epoch 563/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0414\n",
      "Epoch 564/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0409\n",
      "Epoch 565/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0413\n",
      "Epoch 566/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0403\n",
      "Epoch 567/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0404\n",
      "Epoch 568/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0412\n",
      "Epoch 569/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0421\n",
      "Epoch 570/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0419\n",
      "Epoch 571/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0407\n",
      "Epoch 572/1000\n",
      "22/22 [==============================] - 0s 7ms/step - loss: 0.0405\n",
      "Epoch 573/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0411\n",
      "Epoch 574/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0407\n",
      "Epoch 575/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0409\n",
      "Epoch 576/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0404\n",
      "Epoch 577/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0409\n",
      "Epoch 578/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0406\n",
      "Epoch 579/1000\n",
      "22/22 [==============================] - 0s 8ms/step - loss: 0.0429\n",
      "Epoch 580/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0408\n",
      "Epoch 581/1000\n",
      "22/22 [==============================] - 0s 6ms/step - loss: 0.0405\n",
      "Epoch 582/1000\n",
      " 8/22 [=========>....................] - ETA: 0s - loss: 0.0436"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-7ecc3234e554>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'mean_squared_error'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"adam\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkerasXW2VG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkerasYW2VG\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m300\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# 0.9429640018700327\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1096\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m   1097\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1098\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1099\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1100\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    778\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m         \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    805\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 807\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    808\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    809\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2830\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1841\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1842\u001b[0m     \"\"\"\n\u001b[0;32m-> 1843\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1844\u001b[0m         [t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1845\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1921\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1922\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1923\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1924\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1925\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    546\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "adam = keras.optimizers.Adam(lr=0.00005, beta_1=0.9, beta_2=0.999, epsilon=1e-10, decay=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(300, input_dim=300, activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='mean_squared_error', optimizer=\"adam\")\n",
    "\n",
    "model.fit(kerasXW2VG, kerasYW2VG, epochs=1000, batch_size=300, verbose=1)\n",
    "\n",
    "# 0.9429640018700327\n",
    "# 0.8803418803418803\n",
    "# 103"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9457690509583918\n",
      "0.5263157894736842\n",
      "0.08547008547008547\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(kerasYDev, y_pred)\n",
    "precision = precision_score(kerasYDev, y_pred)\n",
    "recall = recall_score(kerasYDev, y_pred)\n",
    "\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.25616103\n",
      "Iteration 2, loss = 0.20833713\n",
      "Iteration 3, loss = 0.21988615\n",
      "Iteration 4, loss = 0.19513966\n",
      "Iteration 5, loss = 0.18433653\n",
      "Iteration 6, loss = 0.18874768\n",
      "Iteration 7, loss = 0.17984454\n",
      "Iteration 8, loss = 0.19068481\n",
      "Iteration 9, loss = 0.17995137\n",
      "Iteration 10, loss = 0.17245068\n",
      "Iteration 11, loss = 0.17142375\n",
      "Iteration 12, loss = 0.18456266\n",
      "Iteration 13, loss = 0.16879960\n",
      "Iteration 14, loss = 0.17171872\n",
      "Iteration 15, loss = 0.17968191\n",
      "Iteration 16, loss = 0.17054836\n",
      "Iteration 17, loss = 0.16755943\n",
      "Iteration 18, loss = 0.16857990\n",
      "Iteration 19, loss = 0.16365501\n",
      "Iteration 20, loss = 0.17612061\n",
      "Iteration 21, loss = 0.16391529\n",
      "Iteration 22, loss = 0.16314799\n",
      "Iteration 23, loss = 0.16181412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/neural_network/_multilayer_perceptron.py:587: UserWarning: Training interrupted by user.\n",
      "  warnings.warn(\"Training interrupted by user.\")\n"
     ]
    }
   ],
   "source": [
    "# mlp = MLPClassifier(max_iter=50)\n",
    "# parameter_space = {\n",
    "#     'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "#     'activation': ['tanh', 'relu', 'sigmoid'],\n",
    "#     'solver': ['sgd', 'adam'],\n",
    "#     'alpha': [0.0001, 0.05],\n",
    "#     'learning_rate': ['constant','adaptive'],\n",
    "#     'verbose': [True]\n",
    "# }\n",
    "# parameter_space2 = {\n",
    "#     'hidden_layer_sizes': [(100,)],\n",
    "#     'activation': ['relu'],\n",
    "#     'solver': ['adam'],\n",
    "#     'alpha': [.05],\n",
    "#     'learning_rate': ['constant'],\n",
    "#     'verbose': [True]\n",
    "# }\n",
    "# clf = GridSearchCV(mlp, parameter_space2, n_jobs=-1, cv=3)\n",
    "clf = MLPClassifier(hidden_layer_sizes=(10000,100,100 ), activation='tanh', alpha=.00001, random_state=None, max_iter=500, verbose=True)\n",
    "\n",
    "clf.fit(kerasXW2VG, kerasYW2VG)\n",
    "y_pred = clf.predict(kerasXDevW2VG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.946236559139785\n",
      "0.5416666666666666\n",
      "0.1111111111111111\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(kerasYDevW2VG, y_pred)\n",
    "precision = precision_score(kerasYDevW2VG, y_pred)\n",
    "recall = recall_score(kerasYDevW2VG, y_pred)\n",
    "\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)\n",
    "\n",
    "# 0.9387564282374942\n",
    "# 0.3870967741935484\n",
    "# 0.20512820512820512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On dev set, if accuracy = 0.9453015427769986, model is predicting all 0s\n",
    "# 0.9503026441541892\n",
    "\n",
    "y_pred_keras = model.predict_proba(kerasXDev)\n",
    "\n",
    "countRight = 0\n",
    "countOf1 = 0\n",
    "\n",
    "predictions = []\n",
    "for i in y_pred_keras:\n",
    "    if i < .5:\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)\n",
    "\n",
    "for k in range(len(predictions)):\n",
    "    if predictions[k] == kerasYDev[k]:\n",
    "        countRight += 1\n",
    "        if kerasYDev[k] == 1:\n",
    "            countOf1 += 1\n",
    "\n",
    "accuracy = countRight/2139\n",
    "homelessAccuracy = countOf1/70\n",
    "\n",
    "print(accuracy)\n",
    "print(homelessAccuracy)\n",
    "print(countOf1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted landscape of our data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "test_data_keras_PCA = pca.fit_transform(kerasXDev)\n",
    "class1 = []\n",
    "class2 = []\n",
    "\n",
    "for a in range(len(kerasXDev)-1):\n",
    "    if predictions[a] == 1:\n",
    "        class1.append(test_data_keras_PCA[a])\n",
    "    else:\n",
    "        class2.append(test_data_keras_PCA[a])\n",
    "                         \n",
    "plt.scatter(*zip(*class2), color='blue')\n",
    "plt.scatter(*zip(*class1), color='red')\n",
    "# plt.xlim(-.4, 1.4)\n",
    "# plt.ylim(-.6, .6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True landscape of our data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "test_data_keras_PCA = pca.fit_transform(kerasXDev)\n",
    "class1 = []\n",
    "class2 = []\n",
    "\n",
    "for a in range(len(kerasYDev)-1):\n",
    "    if kerasYDev[a] == 1:\n",
    "        class1.append(test_data_keras_PCA[a])\n",
    "    else:\n",
    "        class2.append(test_data_keras_PCA[a])\n",
    "                         \n",
    "plt.scatter(*zip(*class2), color='blue')\n",
    "plt.scatter(*zip(*class1), color='red')\n",
    "# plt.xlim(-.4, 1.4)\n",
    "# plt.ylim(-.6, .6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# True landscape of our data\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "train_data_keras_PCA = pca.fit_transform(kerasX)\n",
    "class1 = []\n",
    "class2 = []\n",
    "\n",
    "for a in range(len(kerasY)-1):\n",
    "    if kerasY[a] == 1:\n",
    "        class1.append(test_data_keras_PCA[a])\n",
    "    else:\n",
    "        class2.append(test_data_keras_PCA[a])\n",
    "                         \n",
    "plt.scatter(*zip(*class2), color='blue')\n",
    "plt.scatter(*zip(*class1), color='red')\n",
    "# plt.xlim(-.4, 1.4)\n",
    "# plt.ylim(-.6, .6)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "overallDataX = []\n",
    "\n",
    "for i in range(len(kerasXW2V)):\n",
    "    empty=[]\n",
    "    empty = kerasXW2VG[i]+kerasXW2V[i]\n",
    "    overallDataX.append(empty)\n",
    "    \n",
    "overallDataXDev = []\n",
    "for i in range(len(kerasXDevW2V)):\n",
    "    empty=[]\n",
    "    empty = kerasXDevW2VG[i]+kerasXDevW2V[i]\n",
    "    overallDataXDev.append(empty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(overallDataX[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.41299624\n",
      "Iteration 2, loss = 0.24205692\n",
      "Iteration 3, loss = 0.22432972\n",
      "Iteration 4, loss = 0.20598508\n",
      "Iteration 5, loss = 0.21859361\n",
      "Iteration 6, loss = 0.19745572\n",
      "Iteration 7, loss = 0.18884586\n",
      "Iteration 8, loss = 0.19099534\n",
      "Iteration 9, loss = 0.21217940\n",
      "Iteration 10, loss = 0.18650827\n",
      "Iteration 11, loss = 0.19064141\n",
      "Iteration 12, loss = 0.19462621\n",
      "Iteration 13, loss = 0.18215107\n",
      "Iteration 14, loss = 0.18167902\n",
      "Iteration 15, loss = 0.19710512\n",
      "Iteration 16, loss = 0.18511896\n",
      "Iteration 17, loss = 0.18005635\n",
      "Iteration 18, loss = 0.19329563\n",
      "Iteration 19, loss = 0.17690957\n",
      "Iteration 20, loss = 0.19834587\n",
      "Iteration 21, loss = 0.17913410\n",
      "Iteration 22, loss = 0.17418975\n",
      "Iteration 23, loss = 0.18722590\n",
      "Iteration 24, loss = 0.17687071\n",
      "Iteration 25, loss = 0.17164413\n",
      "Iteration 26, loss = 0.17610248\n",
      "Iteration 27, loss = 0.17357599\n",
      "Iteration 28, loss = 0.17190061\n",
      "Iteration 29, loss = 0.18855105\n",
      "Iteration 30, loss = 0.17286886\n",
      "Iteration 31, loss = 0.17379442\n",
      "Iteration 32, loss = 0.17073707\n",
      "Iteration 33, loss = 0.17846957\n",
      "Iteration 34, loss = 0.17486054\n",
      "Iteration 35, loss = 0.17104397\n",
      "Iteration 36, loss = 0.17908221\n",
      "Iteration 37, loss = 0.18247990\n",
      "Iteration 38, loss = 0.18814914\n",
      "Iteration 39, loss = 0.17591037\n",
      "Iteration 40, loss = 0.16661203\n",
      "Iteration 41, loss = 0.16497565\n",
      "Iteration 42, loss = 0.16827001\n",
      "Iteration 43, loss = 0.17886062\n",
      "Iteration 44, loss = 0.18695720\n",
      "Iteration 45, loss = 0.18141218\n",
      "Iteration 46, loss = 0.16884299\n",
      "Iteration 47, loss = 0.16745377\n",
      "Iteration 48, loss = 0.16512685\n",
      "Iteration 49, loss = 0.16503757\n",
      "Iteration 50, loss = 0.17831005\n",
      "Iteration 51, loss = 0.17596740\n",
      "Iteration 52, loss = 0.17401533\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "clf = MLPClassifier(hidden_layer_sizes=(1000, ), activation='tanh', learning_rate_init=0.005, random_state=None, max_iter=500, verbose=True)\n",
    "\n",
    "clf.fit(overallDataX, kerasYW2V)\n",
    "y_pred = clf.predict(overallDataXDev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9453015427769986\n",
      "0.4444444444444444\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(kerasYDevW2VG, y_pred)\n",
    "precision = precision_score(kerasYDevW2VG, y_pred)\n",
    "recall = recall_score(kerasYDevW2VG, y_pred)\n",
    "\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)\n",
    "\n",
    "# clf = MLPClassifier(hidden_layer_sizes=(500, ), activation='tanh', learning_rate_init=0.009, random_state=None, max_iter=500, verbose=True)\n",
    "\n",
    "# 0.8279569892473119\n",
    "# 0.4444444444444444\n",
    "# 0.8205128205128205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.2146\n",
      "Epoch 2/100\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.1597\n",
      "Epoch 3/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.1236\n",
      "Epoch 4/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.1015\n",
      "Epoch 5/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0879\n",
      "Epoch 6/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0794\n",
      "Epoch 7/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0737\n",
      "Epoch 8/100\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0699\n",
      "Epoch 9/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0672\n",
      "Epoch 10/100\n",
      "26/26 [==============================] - 0s 4ms/step - loss: 0.0653\n",
      "Epoch 11/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0638\n",
      "Epoch 12/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0628\n",
      "Epoch 13/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0619\n",
      "Epoch 14/100\n",
      "26/26 [==============================] - 0s 780us/step - loss: 0.0613\n",
      "Epoch 15/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0607\n",
      "Epoch 16/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0603\n",
      "Epoch 17/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0600\n",
      "Epoch 18/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0597\n",
      "Epoch 19/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0595\n",
      "Epoch 20/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0593\n",
      "Epoch 21/100\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0591\n",
      "Epoch 22/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0590\n",
      "Epoch 23/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0589\n",
      "Epoch 24/100\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0588\n",
      "Epoch 25/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0587\n",
      "Epoch 26/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0586\n",
      "Epoch 27/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0585\n",
      "Epoch 28/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0585\n",
      "Epoch 29/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0584\n",
      "Epoch 30/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0584\n",
      "Epoch 31/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0583\n",
      "Epoch 32/100\n",
      "26/26 [==============================] - 0s 3ms/step - loss: 0.0583\n",
      "Epoch 33/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0583\n",
      "Epoch 34/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0582\n",
      "Epoch 35/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0582\n",
      "Epoch 36/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0582\n",
      "Epoch 37/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0581\n",
      "Epoch 38/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0581\n",
      "Epoch 39/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0581\n",
      "Epoch 40/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0581\n",
      "Epoch 41/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0580\n",
      "Epoch 42/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0580\n",
      "Epoch 43/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0580\n",
      "Epoch 44/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0580\n",
      "Epoch 45/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0579\n",
      "Epoch 46/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0579\n",
      "Epoch 47/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0579\n",
      "Epoch 48/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0579\n",
      "Epoch 49/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0579\n",
      "Epoch 50/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0578\n",
      "Epoch 51/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0578\n",
      "Epoch 52/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0578\n",
      "Epoch 53/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0578\n",
      "Epoch 54/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0578\n",
      "Epoch 55/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0577\n",
      "Epoch 56/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0577\n",
      "Epoch 57/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0577\n",
      "Epoch 58/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0577\n",
      "Epoch 59/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0576\n",
      "Epoch 60/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0576\n",
      "Epoch 61/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0576\n",
      "Epoch 62/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0576\n",
      "Epoch 63/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0576\n",
      "Epoch 64/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0575\n",
      "Epoch 65/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0575\n",
      "Epoch 66/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0575\n",
      "Epoch 67/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0575\n",
      "Epoch 68/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0574\n",
      "Epoch 69/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0574\n",
      "Epoch 70/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0574\n",
      "Epoch 71/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0574\n",
      "Epoch 72/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0573\n",
      "Epoch 73/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0573\n",
      "Epoch 74/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0573\n",
      "Epoch 75/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0573\n",
      "Epoch 76/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0572\n",
      "Epoch 77/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0572\n",
      "Epoch 78/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0572\n",
      "Epoch 79/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0572\n",
      "Epoch 80/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0571\n",
      "Epoch 81/100\n",
      "26/26 [==============================] - 0s 2ms/step - loss: 0.0571\n",
      "Epoch 82/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0571\n",
      "Epoch 83/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0571\n",
      "Epoch 84/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0570\n",
      "Epoch 85/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0570\n",
      "Epoch 86/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0570\n",
      "Epoch 87/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0570\n",
      "Epoch 88/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0569\n",
      "Epoch 89/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0569\n",
      "Epoch 90/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0569\n",
      "Epoch 91/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0568\n",
      "Epoch 92/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0568\n",
      "Epoch 93/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0568\n",
      "Epoch 94/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0568\n",
      "Epoch 95/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0567\n",
      "Epoch 96/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0567\n",
      "Epoch 97/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0567\n",
      "Epoch 98/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0567\n",
      "Epoch 99/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0566\n",
      "Epoch 100/100\n",
      "26/26 [==============================] - 0s 1ms/step - loss: 0.0566\n",
      "0.9453015427769986\n",
      "0.0\n",
      "0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 due to no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "\n",
    "adam = keras.optimizers.Adam(lr=0.005, beta_1=0.9, beta_2=0.999, epsilon=1e-10, decay=0)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(1, input_dim=300, activation='sigmoid'))\n",
    "model.compile(loss='mean_squared_error', optimizer=\"adam\")\n",
    "\n",
    "model.fit(kerasXW2VG, kerasYW2VG, epochs=100, batch_size=250)\n",
    "y_pred=model.predict(kerasXDevW2VG)\n",
    "preds=[]\n",
    "for i in y_pred:\n",
    "    if i[0] > .5:\n",
    "        preds.append(1)\n",
    "    else:   \n",
    "        preds.append(0)\n",
    "accuracy(preds, kerasYDevW2VG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9476390836839644\n",
      "0.08547008547008547\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "for i in y_pred:\n",
    "    if i < .5:\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)\n",
    "\n",
    "accuracy = accuracy_score(kerasYDevW2VG, predictions)\n",
    "# precision_score = precision_score(kerasYDevW2VG, predictions)\n",
    "recall = recall_score(kerasYDevW2VG, predictions)\n",
    "\n",
    "print(accuracy)\n",
    "# print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.05190447, 0.048850864, 0.051793218, 0.058837682, 0.058488905, 0.048933953, 0.059798837, 0.036573887, 0.047164887, 0.11070764, 0.06680304, 0.05262333, 0.053738177, 0.06020829, 0.057163805, 0.046044797, 0.094379604, 0.05159211, 0.10232422, 0.054990083, 0.061270922, 0.063501894, 0.059171736, 0.05167553, 0.06019509, 0.12695673, 0.05048418, 0.04174769, 0.050454855, 0.08191979, 0.056955844, 0.10240689, 0.07495022, 0.048793614, 0.07581258, 0.039911777, 0.08269456, 0.05784583, 0.051147938, 0.048368633, 0.050542325, 0.058942914, 0.04372579, 0.053551733, 0.05082959, 0.09808704, 0.05892098, 0.086536944, 0.047336638, 0.044496775, 0.073434144, 0.08322695, 0.06321496, 0.060032368, 0.063346505, 0.057533205, 0.084082484, 0.056800455, 0.048806846, 0.06578469, 0.051249146, 0.08184478, 0.07232675, 0.07619488, 0.06673145, 0.07798937, 0.05144617, 0.06643531, 0.062042594, 0.091266036, 0.08244115, 0.06131506, 0.050387353, 0.08990711, 0.052901268, 0.091807246, 0.09511432, 0.051769555, 0.06888148, 0.12635806, 0.068911165, 0.095508695, 0.056134284, 0.062827975, 0.08590898, 0.060267657, 0.10364911, 0.05379057, 0.10753971, 0.09059861, 0.0595378, 0.05584863, 0.12296024, 0.049243063, 0.08273521, 0.05507174, 0.09338689, 0.09588069, 0.08533755, 0.113262415, 0.066878945, 0.121510476, 0.071250975, 0.0814254, 0.08675262, 0.052111447, 0.06267542, 0.07097998, 0.099663794, 0.05993998, 0.046405226, 0.06749958, 0.06641248, 0.056925505, 0.06395736, 0.061032325, 0.0554083, 0.05691713, 0.05098191, 0.052224725, 0.06671795, 0.11435482, 0.11347774, 0.06538862, 0.051365584, 0.07050237, 0.05050358, 0.10006362, 0.103410274, 0.051695436, 0.059159756, 0.04253137, 0.10884559, 0.05883631, 0.12993303, 0.06534609, 0.057566345, 0.092193455, 0.053183645, 0.057573706, 0.097053885, 0.062244028, 0.06790769, 0.099844694, 0.07566905, 0.0730876, 0.050070792, 0.07585618, 0.053047568, 0.09233001, 0.07030183, 0.044866383, 0.061156005, 0.10210812, 0.06197965, 0.062430233, 0.10681, 0.079808176, 0.08977413, 0.09702486, 0.09637448, 0.044765234, 0.078830004, 0.06071022, 0.053783566, 0.057011455, 0.098674744, 0.046262354, 0.04408115, 0.06327346, 0.06249112, 0.046564132, 0.13929746, 0.091968715, 0.05260417, 0.060979962, 0.10314575, 0.07056007, 0.11174849, 0.067489386, 0.091089666, 0.08020136, 0.05093181, 0.05458677, 0.07065052, 0.056588262, 0.054396838, 0.06377146, 0.0614627, 0.06387296, 0.09865397, 0.051075697, 0.05826208, 0.07623759, 0.06610915, 0.062879324, 0.094258934, 0.084644616, 0.06742233, 0.0726355, 0.05497086, 0.0828473, 0.04962948, 0.08413017, 0.077305436, 0.09810215, 0.096192956, 0.08217907, 0.09477851, 0.109351724, 0.044253886, 0.04936406, 0.089268655, 0.05110559, 0.103363454, 0.06585151, 0.042011738, 0.05456394, 0.10236159, 0.102303326, 0.05948153, 0.07311848, 0.097546786, 0.051768064, 0.056416392, 0.10224214, 0.061928034, 0.054896504, 0.09391144, 0.08255398, 0.12070617, 0.061630458, 0.110889554, 0.06932598, 0.08938503, 0.056114614, 0.09387612, 0.053791583, 0.07429105, 0.11371204, 0.0653272, 0.05640176, 0.08041915, 0.0564844, 0.06334445, 0.14588052, 0.092010826, 0.047219217, 0.06599933, 0.064799786, 0.05594468, 0.06976265, 0.14788163, 0.052138895, 0.10134485, 0.053800315, 0.09714067, 0.105873704, 0.051273346, 0.07256338, 0.080221504, 0.05487737, 0.113372475, 0.05421436, 0.0662657, 0.09421778, 0.067091405, 0.086452216, 0.062925756, 0.07377851, 0.062444597, 0.0916827, 0.05207184, 0.06897184, 0.093723476, 0.11770901, 0.044048965, 0.06910172, 0.09539154, 0.08550641, 0.08184898, 0.048877418, 0.055184633, 0.10129103, 0.07394156, 0.108150244, 0.10622212, 0.10032788, 0.05785051, 0.064240545, 0.079173654, 0.0784381, 0.09130731, 0.044111967, 0.051882416, 0.06664932, 0.07648602, 0.043320954, 0.05216065, 0.095342994, 0.08656055, 0.12030852, 0.048770398, 0.057079107, 0.04982239, 0.050709873, 0.06244394, 0.066060215, 0.075676054, 0.07564625, 0.049031943, 0.041562766, 0.054184914, 0.063162535, 0.059529424, 0.045140296, 0.050209016, 0.09179723, 0.052915275, 0.038739026, 0.055435658, 0.042708546, 0.06327587, 0.11795351, 0.059386253, 0.052886337, 0.066268414, 0.058045894, 0.06405482, 0.0544385, 0.06942454, 0.05558473, 0.11290461, 0.071579814, 0.050560623, 0.11003989, 0.064062506, 0.07627699, 0.084985495, 0.051268905, 0.13035172, 0.04957658, 0.052194983, 0.06373966, 0.07935718, 0.04953575, 0.09812024, 0.07293713, 0.032393903, 0.114194214, 0.07113281, 0.061721504, 0.060460478, 0.09364772, 0.0838314, 0.08928612, 0.07027343, 0.08366534, 0.04754743, 0.073518455, 0.11196986, 0.066048056, 0.05775714, 0.06618455, 0.053067267, 0.1047101, 0.068077475, 0.11700153, 0.11028567, 0.045841724, 0.06275469, 0.0622831, 0.05819258, 0.0763087, 0.044623047, 0.044411153, 0.034794837, 0.05104214, 0.040434003, 0.072562814, 0.10602054, 0.058238775, 0.062944025, 0.12201241, 0.052068055, 0.071294785, 0.050844103, 0.051594645, 0.08524531, 0.051663637, 0.099015474, 0.07332724, 0.051605463, 0.04806575, 0.09720048, 0.096265435, 0.047300637, 0.080848634, 0.060116976, 0.10867953, 0.048940808, 0.053081483, 0.074272186, 0.060596287, 0.05845484, 0.07145411, 0.049487203, 0.05943328, 0.09919903, 0.08456996, 0.06423107, 0.05730763, 0.04747653, 0.07599938, 0.053494543, 0.04717958, 0.05185193, 0.121596396, 0.044125557, 0.05033958, 0.06316215, 0.061252296, 0.058858424, 0.05956745, 0.050240695, 0.0794557, 0.054260373, 0.09899461, 0.05668679, 0.072609216, 0.06530687, 0.06368977, 0.061582506, 0.046331465, 0.08569303, 0.05433601, 0.06253263, 0.15098521, 0.074103355, 0.06709507, 0.053201348, 0.120517045, 0.052101403, 0.10379854, 0.049217403, 0.09533638, 0.07166219, 0.08734125, 0.06150925, 0.06969237, 0.047721058, 0.0759646, 0.06348339, 0.05362615, 0.047904134, 0.06657031, 0.063663065, 0.06763792, 0.05391109, 0.04277709, 0.080509275, 0.06251407, 0.06953877, 0.054438174, 0.065744996, 0.10940197, 0.08904454, 0.048932046, 0.052204937, 0.05704409, 0.08924943, 0.093481004, 0.09641647, 0.059246004, 0.07403621, 0.04525131, 0.10220298, 0.03994155, 0.06093657, 0.0685938, 0.06816834, 0.102038115, 0.044607908, 0.08162835, 0.0814828, 0.078817844, 0.06516728, 0.121276885, 0.07716623, 0.049666435, 0.045681536, 0.088365525, 0.09453422, 0.09288001, 0.060498327, 0.040892422, 0.07691258, 0.06702113, 0.051021665, 0.06706965, 0.07592842, 0.06198415, 0.1136893, 0.05901259, 0.0653781, 0.05039832, 0.05682999, 0.092507094, 0.05239418, 0.07762739, 0.058671176, 0.055568516, 0.064543515, 0.06313515, 0.054749966, 0.049949795, 0.049853057, 0.06223166, 0.05258754, 0.09096143, 0.058749318, 0.08634934, 0.07880497, 0.10091984, 0.061745644, 0.09347305, 0.049925268, 0.056882948, 0.07617542, 0.09492418, 0.04308009, 0.051223934, 0.055470526, 0.07084015, 0.065333426, 0.07018167, 0.051792502, 0.043784678, 0.04269615, 0.06446275, 0.053261787, 0.07710263, 0.09741551, 0.07031596, 0.10141483, 0.053913087, 0.048341215, 0.057071567, 0.11363122, 0.08334005, 0.12593445, 0.04896912, 0.11699894, 0.071520954, 0.092915714, 0.06538227, 0.09617853, 0.056939244, 0.052132666, 0.057946384, 0.0632239, 0.06921303, 0.057566673, 0.053031743, 0.094616175, 0.049488932, 0.046999097, 0.114260346, 0.055572867, 0.07241148, 0.13401967, 0.08148956, 0.056331336, 0.1437858, 0.06009513, 0.04573667, 0.0468989, 0.06956276, 0.06841117, 0.0831525, 0.112921715, 0.070822, 0.06261033, 0.104019165, 0.055429786, 0.10631016, 0.06866637, 0.066081405, 0.080828846, 0.08310029, 0.040897608, 0.06358385, 0.10019079, 0.051769346, 0.049082488, 0.07125899, 0.047726274, 0.06481165, 0.05101055, 0.0632948, 0.06631845, 0.054252, 0.107370436, 0.048247278, 0.06147206, 0.058182, 0.05621913, 0.058854133, 0.058346123, 0.09044346, 0.0643982, 0.108095884, 0.059782654, 0.05353886, 0.08001968, 0.063308, 0.05808842, 0.062233716, 0.065745175, 0.09197223, 0.09493825, 0.07491672, 0.08535075, 0.16357043, 0.095300525, 0.060514897, 0.095710546, 0.053260446, 0.07937142, 0.03975725, 0.05557567, 0.094382435, 0.09360129, 0.04852748, 0.04956925, 0.063218206, 0.08535868, 0.07694557, 0.05657372, 0.0818277, 0.049028248, 0.06439844, 0.051279932, 0.060949773, 0.115538746, 0.105578035, 0.05273235, 0.11182833, 0.06389615, 0.072445124, 0.078623205, 0.07791576, 0.041508555, 0.078722894, 0.047649592, 0.110630095, 0.06193933, 0.049319953, 0.06886068, 0.09108597, 0.099562734, 0.084012896, 0.04815346, 0.05524817, 0.053271025, 0.09873584, 0.052925736, 0.09751251, 0.05693677, 0.06717041, 0.06399137, 0.061275452, 0.052668035, 0.060402125, 0.08304092, 0.06292164, 0.043504298, 0.09601289, 0.06450638, 0.10060918, 0.09489599, 0.048931926, 0.05670324, 0.087452024, 0.071912795, 0.04848215, 0.07589817, 0.08705783, 0.076717526, 0.05995518, 0.059505075, 0.05403772, 0.07428935, 0.0643996, 0.05132389, 0.08372322, 0.06163767, 0.055590212, 0.06550163, 0.05995977, 0.05798939, 0.084181726, 0.052837968, 0.07102138, 0.0669629, 0.06330818, 0.07726273, 0.064706326, 0.041834265, 0.081768125, 0.09509042, 0.094958216, 0.06768781, 0.08491039, 0.09453395, 0.047654063, 0.08497527, 0.039800376, 0.105938494, 0.056179285, 0.109190255, 0.08965105, 0.06591806, 0.07267359, 0.04919699, 0.089408904, 0.119671255, 0.0912483, 0.08995709, 0.08286524, 0.065342695, 0.057325393, 0.0680303, 0.09695503, 0.04690668, 0.06698865, 0.12096894, 0.093654275, 0.09708026, 0.06585914, 0.05430627, 0.12777135, 0.10854632, 0.049961895, 0.052406013, 0.07078689, 0.050720543, 0.086818546, 0.06406337, 0.071766734, 0.05688557, 0.042419612, 0.07025188, 0.11331898, 0.04632309, 0.07213876, 0.11315477, 0.060883403, 0.057252467, 0.059915364, 0.06675121, 0.054399252, 0.07605171, 0.06923285, 0.08181381, 0.08939898, 0.1178236, 0.06829983, 0.09966567, 0.10293117, 0.07320449, 0.0392493, 0.08707774, 0.09259471, 0.07256478, 0.101706475, 0.049814075, 0.11012015, 0.09797472, 0.078499675, 0.09787333, 0.041546345, 0.087605715, 0.11676273, 0.06392443, 0.10534829, 0.10761917, 0.0976809, 0.07785389, 0.068752885, 0.060447097, 0.09607968, 0.11907041, 0.074037045, 0.08073151, 0.04462722, 0.08932459, 0.06301224, 0.087822646, 0.058272153, 0.08211249, 0.040326387, 0.047554374, 0.04118201, 0.05284792, 0.089842945, 0.09759495, 0.11336264, 0.08490685, 0.051941484, 0.09587982, 0.057188243, 0.050720066, 0.09277317, 0.09074816, 0.090797454, 0.06826979, 0.08506292, 0.07626051, 0.052866727, 0.08036384, 0.05258733, 0.09837198, 0.09551853, 0.096179724, 0.07983023, 0.05341661, 0.054531217, 0.10792896, 0.060637772, 0.06429705, 0.074300945, 0.0740526, 0.07601297, 0.10432267, 0.04913336, 0.05126497, 0.07580644, 0.047396928, 0.08090174, 0.0662781, 0.07785705, 0.10323557, 0.07131767, 0.055306822, 0.054288477, 0.13004243, 0.06622654, 0.078626364, 0.07424781, 0.102657706, 0.052808374, 0.09394351, 0.13025492, 0.053770185, 0.10636613, 0.043370783, 0.090754926, 0.05445817, 0.05465567, 0.057382405, 0.0798974, 0.068927675, 0.0457893, 0.040995717, 0.088662386, 0.071526915, 0.061074376, 0.051665127, 0.047844946, 0.103283346, 0.052756846, 0.07583684, 0.070299596, 0.054205567, 0.05338493, 0.058326542, 0.052127242, 0.04812759, 0.056883663, 0.10867271, 0.0599249, 0.078323126, 0.10582897, 0.10743755, 0.068119794, 0.099743396, 0.03687617, 0.06825602, 0.0723362, 0.050599158, 0.08474004, 0.08278254, 0.07322246, 0.05593598, 0.0685204, 0.09839314, 0.053035885, 0.06568152, 0.088631034, 0.0816035, 0.12076536, 0.051618785, 0.09461349, 0.06342274, 0.08046374, 0.05002895, 0.059005797, 0.10504538, 0.07211316, 0.068519324, 0.09658727, 0.053963363, 0.05557871, 0.057263553, 0.065784544, 0.052525252, 0.10484481, 0.11105782, 0.03668624, 0.072942436, 0.06643182, 0.08846888, 0.06143853, 0.11277777, 0.09359285, 0.08971146, 0.059421718, 0.056665182, 0.12538978, 0.07232675, 0.11258814, 0.07301882, 0.080765545, 0.07032636, 0.059381723, 0.042415738, 0.06562996, 0.08916369, 0.100783885, 0.05916792, 0.08638212, 0.05096793, 0.0707297, 0.04883024, 0.06273478, 0.051296115, 0.068784446, 0.053868443, 0.04897648, 0.06838325, 0.0686416, 0.070843995, 0.10484627, 0.097934276, 0.10190016, 0.059005767, 0.10876566, 0.077504516, 0.053970248, 0.048369408, 0.0767445, 0.059029907, 0.057458192, 0.06735107, 0.08515203, 0.072506934, 0.054469794, 0.04153472, 0.0712384, 0.06807542, 0.050958127, 0.092421025, 0.047036916, 0.10500023, 0.04938343, 0.095112026, 0.049423218, 0.04950887, 0.06681964, 0.1329185, 0.07313636, 0.07951921, 0.11594844, 0.1477103, 0.10482633, 0.117900044, 0.050147384, 0.051501304, 0.101079166, 0.05852264, 0.059093207, 0.04158458, 0.064380735, 0.06913349, 0.08480847, 0.06278542, 0.09377682, 0.08303997, 0.050718695, 0.08935791, 0.075715125, 0.04855907, 0.0496341, 0.0713523, 0.064560324, 0.051605552, 0.072442114, 0.06003037, 0.08712268, 0.068083346, 0.071413845, 0.047863662, 0.094480306, 0.07200652, 0.06499687, 0.06458008, 0.07840717, 0.09374973, 0.061055243, 0.09112498, 0.06975317, 0.080569744, 0.05615005, 0.042697936, 0.055097103, 0.050004452, 0.10798782, 0.031227678, 0.07166496, 0.073955625, 0.060786337, 0.066330165, 0.036468685, 0.09443024, 0.06281322, 0.066417605, 0.06608021, 0.07088566, 0.05361882, 0.08350009, 0.050626695, 0.093628794, 0.07034391, 0.1155135, 0.049390048, 0.09840429, 0.09367731, 0.08588475, 0.07862121, 0.050362647, 0.05614552, 0.048042238, 0.047054857, 0.09133482, 0.06313315, 0.07392439, 0.049390197, 0.08457738, 0.08571643, 0.046852976, 0.07204229, 0.0721322, 0.065719694, 0.05225581, 0.051655054, 0.061386377, 0.06896302, 0.056150764, 0.067563504, 0.110073626, 0.056858152, 0.050993025, 0.08402252, 0.057909906, 0.08520055, 0.062269002, 0.06437203, 0.058102548, 0.08910835, 0.06821129, 0.044744402, 0.046708494, 0.07218611, 0.06723082, 0.07071307, 0.05395806, 0.105373144, 0.05441913, 0.0572083, 0.060632616, 0.12283638, 0.073816836, 0.06254846, 0.048858285, 0.09490171, 0.09003425, 0.056191295, 0.105216235, 0.08343026, 0.062481165, 0.06523967, 0.07318175, 0.075623155, 0.0612154, 0.061961472, 0.054623097, 0.059819013, 0.068140805, 0.043722123, 0.062438607, 0.05205643, 0.051558226, 0.047776848, 0.077582866, 0.07270655, 0.097179234, 0.08585644, 0.051434606, 0.09290165, 0.052206784, 0.082508326, 0.11559263, 0.04598266, 0.064070344, 0.06837979, 0.068353534, 0.042285174, 0.07530218, 0.102464736, 0.076284975, 0.03968668, 0.08978903, 0.08427626, 0.10863042, 0.115805686, 0.05242282, 0.10928005, 0.058407664, 0.057908297, 0.05703646, 0.08571857, 0.052519113, 0.068069994, 0.14122781, 0.079910636, 0.06782085, 0.13202798, 0.050492883, 0.067616224, 0.06775093, 0.12319666, 0.05852762, 0.12309599, 0.09350249, 0.0674178, 0.06265128, 0.093183875, 0.06665707, 0.06909868, 0.050344944, 0.06302369, 0.097586274, 0.10673201, 0.05493185, 0.05111766, 0.047525644, 0.08500016, 0.0619376, 0.06342554, 0.06776196, 0.06636411, 0.06945929, 0.05776015, 0.051084876, 0.054912537, 0.093791604, 0.09569043, 0.045620263, 0.050934076, 0.082187235, 0.048779875, 0.08523929, 0.0964576, 0.04656735, 0.065424204, 0.09332198, 0.10062647, 0.07087502, 0.0769043, 0.06065744, 0.053981245, 0.08119506, 0.044757336, 0.05025217, 0.07399237, 0.054505736, 0.06439182, 0.114008605, 0.057240814, 0.053260118, 0.042242855, 0.046105593, 0.06273821, 0.094665855, 0.06798196, 0.04783857, 0.065901905, 0.056507558, 0.056113154, 0.055718005, 0.047379225, 0.08711931, 0.09430927, 0.07027188, 0.053904623, 0.057049304, 0.07560095, 0.060956687, 0.07094926, 0.083426505, 0.0845927, 0.04538873, 0.06101787, 0.06596485, 0.04457292, 0.07228133, 0.11069092, 0.117231846, 0.08136734, 0.106912166, 0.088478416, 0.07528919, 0.0785813, 0.058049053, 0.06314173, 0.09915751, 0.045151263, 0.07423517, 0.053392023, 0.04507169, 0.06575018, 0.099374294, 0.0797776, 0.0682812, 0.058303118, 0.08350974, 0.06964341, 0.117666245, 0.046390325, 0.044606447, 0.06732181, 0.065842986, 0.062378973, 0.04943803, 0.0913257, 0.04695061, 0.07726851, 0.08372575, 0.09413913, 0.06819311, 0.10684356, 0.059056073, 0.060768634, 0.09641287, 0.045604706, 0.066707134, 0.046561033, 0.09153652, 0.09391403, 0.06476018, 0.04619673, 0.09811321, 0.064433664, 0.053285778, 0.046612322, 0.05611837, 0.051005214, 0.07971984, 0.06799945, 0.056331187, 0.06975818, 0.06785753, 0.10060716, 0.06406969, 0.08571845, 0.055966556, 0.10887319, 0.06332308, 0.08291271, 0.07451609, 0.08691546, 0.0959737, 0.09263119, 0.0675706, 0.048313797, 0.12176007, 0.057171613, 0.09298217, 0.068980634, 0.066779226, 0.06869832, 0.080473155, 0.058930755, 0.053414017, 0.09097421, 0.054573447, 0.056667805, 0.04717374, 0.118124425, 0.09597978, 0.11983001, 0.07104275, 0.050255418, 0.079022855, 0.056898683, 0.057996303, 0.042534202, 0.056230158, 0.14305225, 0.09900957, 0.0784671, 0.076114416, 0.036893576, 0.07114312, 0.06872958, 0.10257101, 0.10361838, 0.09234983, 0.09166339, 0.069298446, 0.054699123, 0.052906483, 0.043115675, 0.09554383, 0.058237612, 0.06864077, 0.09029162, 0.052129, 0.075660735, 0.070079714, 0.07080889, 0.09118146, 0.10253775, 0.06254852, 0.08257303, 0.1009469, 0.10004488, 0.07638261, 0.057007283, 0.0795455, 0.06764674, 0.10187462, 0.052232355, 0.052814007, 0.07396701, 0.07160607, 0.062121958, 0.06531733, 0.06069562, 0.06898737, 0.11199424, 0.09415069, 0.049518585, 0.047132492, 0.09114072, 0.052025437, 0.034876883, 0.07952866, 0.05908695, 0.057241887, 0.06835055, 0.03953746, 0.09650272, 0.06023273, 0.100934595, 0.0441615, 0.06387153, 0.09056842, 0.064316094, 0.054928303, 0.05650443, 0.04888937, 0.10731214, 0.07602286, 0.052827805, 0.107076645, 0.051757425, 0.047596365, 0.046145916, 0.06402469, 0.07049331, 0.05768913, 0.06586391, 0.10833803, 0.08278829, 0.0970428, 0.04837218, 0.045556128, 0.042966127, 0.07131976, 0.06688675, 0.096146494, 0.069919705, 0.04809615, 0.077961296, 0.06300187, 0.04862511, 0.053275675, 0.048785925, 0.06501117, 0.06682873, 0.10757449, 0.049910605, 0.10019836, 0.10830596, 0.043328643, 0.0893113, 0.10232565, 0.06386602, 0.06394589, 0.03826776, 0.08996189, 0.06766114, 0.08950317, 0.15435827, 0.04825723, 0.09826183, 0.091881335, 0.073670626, 0.063578695, 0.10421637, 0.07412848, 0.046850234, 0.0605267, 0.034003913, 0.0578475, 0.10192311, 0.06438708, 0.09785017, 0.06506187, 0.08650878, 0.056586146, 0.064245135, 0.05471778, 0.04987994, 0.12169954, 0.079184204, 0.053907424, 0.0878728, 0.07781732, 0.070917696, 0.08815059, 0.055742055, 0.07072115, 0.06924856, 0.0464063, 0.07696912, 0.09300426, 0.053429037, 0.06046897, 0.061623663, 0.0862546, 0.09584367, 0.07304394, 0.062808484, 0.037200093, 0.05263582, 0.051003426, 0.1128636, 0.060442448, 0.0716005, 0.09521398, 0.06357056, 0.07360378, 0.0431872, 0.09452769, 0.09723255, 0.13253838, 0.060259044, 0.04676652, 0.084085315, 0.082936406, 0.065710545, 0.053467184, 0.11223066, 0.1022954, 0.0862039, 0.091773, 0.06916365, 0.10040963, 0.078412056, 0.06355333, 0.052611172, 0.07329425, 0.048452348, 0.103099585, 0.110859394, 0.06708938, 0.06915742, 0.075636566, 0.05788672, 0.049050093, 0.060672462, 0.06360036, 0.055738628, 0.09485218, 0.07682964, 0.060409427, 0.077017784, 0.14247403, 0.07181898, 0.060247272, 0.048950583, 0.052961558, 0.056753814, 0.097490996, 0.045621544, 0.06465939, 0.08840352, 0.13842788, 0.06716949, 0.072583854, 0.072475225, 0.067245394, 0.06722829, 0.0689162, 0.098044574, 0.056172073, 0.07783744, 0.065119326, 0.07025674, 0.09748927, 0.0961718, 0.054480165, 0.10697484, 0.078070015, 0.053119034, 0.060219705, 0.11290121, 0.08327621, 0.06886068, 0.07189414, 0.05504045, 0.05158168, 0.06114924, 0.061898172, 0.099049896, 0.061341316, 0.049821824, 0.10591534, 0.09585303, 0.061674446, 0.101501465, 0.056850016, 0.04963845, 0.05226308, 0.044565737, 0.09918797, 0.057901084, 0.048778623, 0.05209148, 0.095386654, 0.06745908, 0.049711525, 0.057014853, 0.07763228, 0.08602375, 0.07254222, 0.05264011, 0.06941894, 0.13454399, 0.15293798, 0.051934034, 0.108351916, 0.08967748, 0.052197248, 0.063602895, 0.06058696, 0.0789178, 0.056898355, 0.06471127, 0.045964837, 0.0629752, 0.04759711, 0.096416175, 0.06721902, 0.049498737, 0.052138567, 0.056342304, 0.06538227, 0.06638554, 0.07138115, 0.06006825, 0.043254375, 0.049229026, 0.039774537, 0.11654019, 0.051487476, 0.09492788, 0.097506434, 0.059490263, 0.06351322, 0.050077617, 0.05465105, 0.054098666, 0.058683306, 0.0585939, 0.09500685, 0.052158594, 0.0714502, 0.06452662, 0.07527658, 0.08502555, 0.06792927, 0.07457164, 0.0753943, 0.055479318, 0.07717109, 0.10989767, 0.11466661, 0.11190966, 0.08733481, 0.10489631, 0.07702112, 0.064031065, 0.06414914, 0.08992681, 0.09746733, 0.06885433, 0.04542148, 0.037493944, 0.07855648, 0.08444697, 0.08349913, 0.099874675, 0.06496, 0.028623343, 0.09302431, 0.1074743, 0.06541908, 0.07373962, 0.09507233, 0.10690972, 0.054683477, 0.06764066, 0.10312337, 0.040745646, 0.05292827, 0.14293498, 0.06261155, 0.04736942, 0.10044679, 0.10257366, 0.05340627, 0.07763383, 0.07080996, 0.05904892, 0.06597623, 0.043869495, 0.12712556, 0.06673923, 0.06624353, 0.085188925, 0.07221636, 0.08311966, 0.06952047, 0.0705457, 0.0809758, 0.14659673, 0.07494408, 0.050793946, 0.058422238, 0.072601676, 0.05115825, 0.07162976, 0.08481911, 0.105846286, 0.057602018, 0.059880555, 0.059596777, 0.04143536, 0.050280303, 0.0980126, 0.114848286, 0.13224465, 0.037132323, 0.09432024, 0.09906575, 0.10620868, 0.10519406, 0.044654727, 0.07345992, 0.09271434, 0.050941408, 0.10902813, 0.11864689, 0.10758203, 0.05351469, 0.10716924, 0.050247073, 0.07095948, 0.047938168, 0.07422617, 0.059363216, 0.07418829, 0.1331968, 0.047121614, 0.088670224, 0.08164841, 0.07101774, 0.09925547, 0.045656443, 0.07175499, 0.072837174, 0.11520243, 0.11996177, 0.08333224, 0.054717243, 0.11201081, 0.075069755, 0.084127456, 0.057201356, 0.08652717, 0.11587036, 0.08951917, 0.08584148, 0.10439703, 0.057342082, 0.06976169, 0.055414647, 0.09118047, 0.08273727, 0.04329583, 0.1209003, 0.13163683, 0.095048636, 0.09323588, 0.10343468, 0.057240576, 0.047682136, 0.072609365, 0.05848086, 0.07093644, 0.04393512, 0.1088931, 0.060879648, 0.09834951, 0.076007545, 0.10958311, 0.09707105, 0.05829048, 0.08286035, 0.058921307, 0.11463156, 0.053500295, 0.101471335, 0.09800118, 0.055645764, 0.06261775, 0.08606887, 0.08274886, 0.060487688, 0.10951722, 0.082639575, 0.054249972, 0.090697944, 0.073352575, 0.052446514, 0.060709, 0.06517559, 0.06677222, 0.06213975, 0.11132339, 0.06715214, 0.121993005, 0.06896612, 0.091930985, 0.09028554, 0.05406481, 0.047578752, 0.0760079, 0.08707082, 0.0716449, 0.07286307, 0.049551934, 0.07255554, 0.06694004, 0.06591877, 0.040018618, 0.112829864, 0.07472077, 0.05848834, 0.07886866, 0.06845629, 0.08643088, 0.10959417, 0.04585281, 0.06745693, 0.09686166, 0.057963848, 0.047850788, 0.05904126, 0.124457896, 0.0730589, 0.07148889, 0.05173427, 0.09957707, 0.09482965, 0.074180424, 0.067138106, 0.072702885, 0.05701828, 0.07709241, 0.049911916, 0.06537303, 0.06366262, 0.06320575, 0.04626769, 0.07742596, 0.106806934, 0.09924355, 0.07638267, 0.092894584, 0.06716004, 0.06648907, 0.059068978, 0.0622389, 0.078100085, 0.11560902, 0.06002018, 0.07458517, 0.101293534, 0.091420054, 0.06353304, 0.10461667, 0.061868638, 0.059237868, 0.04531619, 0.06595966, 0.08842939, 0.06730744, 0.07317424, 0.07809484, 0.05399695, 0.07840347, 0.09837055, 0.1411323, 0.042618066, 0.09073487, 0.0865666, 0.07595816, 0.06077978, 0.062515646, 0.05752638, 0.09169036, 0.09107092, 0.057611853, 0.058023244, 0.059477806, 0.082028896, 0.0923982, 0.053512514, 0.04204282, 0.054026127, 0.10542935, 0.07361719, 0.06969449, 0.071792185, 0.06591171, 0.10841411, 0.122130364, 0.04852456, 0.0666655, 0.044819385, 0.08369073, 0.05987975, 0.10341662, 0.07019606, 0.07234207, 0.05901748, 0.055702627, 0.0988864, 0.069586754, 0.05976072, 0.050334632, 0.12779507, 0.061319083, 0.07754409, 0.11190751, 0.08669439, 0.10911402, 0.0694406, 0.037691504, 0.07887778, 0.13964, 0.08075377, 0.053982794, 0.056435257, 0.08340284, 0.096520126, 0.099719465, 0.053890914, 0.05639705, 0.05773583, 0.11102259, 0.09762213, 0.048759848, 0.04505074, 0.054118752, 0.0599429, 0.0697279, 0.094597965, 0.13344279, 0.04462233, 0.053496093, 0.11536634, 0.05902961, 0.04587385, 0.13567853, 0.13008696, 0.058597147, 0.034793854, 0.10886434, 0.061734498, 0.058550954, 0.056409717, 0.057966202, 0.060065657, 0.052918255, 0.059259176, 0.08339828, 0.1321482, 0.0565165, 0.14020085, 0.04561788, 0.13417628, 0.056222886, 0.044789344, 0.11114919, 0.07795781, 0.04717478, 0.05289325, 0.065096766, 0.048984587, 0.04720381, 0.062135458, 0.08728433, 0.07902157, 0.13484389, 0.06774023, 0.093776315, 0.046774834, 0.072888255, 0.06413323, 0.07167685, 0.11482689, 0.05688119, 0.095125735, 0.107123226, 0.104395896, 0.10289621, 0.049509525, 0.04370013, 0.054244936, 0.0625529, 0.047057062, 0.06717381, 0.1031501, 0.10596728, 0.07713148, 0.054482132, 0.043976337, 0.06723276, 0.058202893, 0.0785124, 0.097221345, 0.055110633, 0.050299287, 0.065449685, 0.042519957, 0.07121307, 0.1377207, 0.10073039, 0.071831286, 0.06540924, 0.06738627, 0.067118764, 0.05496642, 0.13932633, 0.07246384, 0.0617688, 0.04598725, 0.0898546, 0.06161931, 0.05910942, 0.051986307, 0.05171275, 0.07370052, 0.09780732, 0.058377236, 0.06650001, 0.0735251, 0.06647846, 0.04611951, 0.054750293, 0.09235343, 0.07487705, 0.055226803, 0.11384308, 0.069188714, 0.12396991, 0.0669803, 0.058135808, 0.059560567, 0.071103245, 0.10888392, 0.0753696, 0.09647432, 0.092990965, 0.04738149, 0.09759289, 0.061326146, 0.079916954, 0.050493717, 0.098400086, 0.09671515, 0.065351635, 0.060608357, 0.05899757, 0.098763585, 0.0457792, 0.08113167, 0.052639604, 0.08650127, 0.085884154, 0.092859924, 0.06067258, 0.124616235, 0.075093, 0.054852664, 0.10242173, 0.05135575, 0.09570384, 0.08936626, 0.10010359, 0.073227346, 0.09462556, 0.115486145, 0.08148086, 0.07317859, 0.07754716, 0.06773201, 0.05789706, 0.06371558, 0.07008523, 0.047896117, 0.057034165, 0.16626427, 0.051920146, 0.050701052, 0.07569358, 0.08201134, 0.06064585, 0.054479152, 0.111228555, 0.0577026, 0.045077503, 0.06676, 0.107079566, 0.055041045, 0.05045858, 0.06018272, 0.07487318, 0.10697317, 0.09221518, 0.06488544, 0.06674495, 0.059488386, 0.101392865, 0.09680441, 0.10665843, 0.05378154, 0.111296564, 0.14305964, 0.04635036, 0.100578845, 0.067664415, 0.107286364, 0.13162374, 0.054317713, 0.07883021, 0.047774285, 0.10117033, 0.06973311, 0.12139019, 0.05141002, 0.037703276, 0.07827029, 0.06228435, 0.1019049, 0.058505446, 0.08748147, 0.09021518, 0.077242166, 0.08069402, 0.08049786, 0.12536898, 0.06296754, 0.090886176, 0.053858757, 0.09682158, 0.057476312, 0.08803564, 0.10973582, 0.12001544, 0.08609083, 0.14193317, 0.10131061, 0.077948004, 0.06643897, 0.056927383, 0.10843012, 0.096577436, 0.07242292, 0.10974711, 0.08853179, 0.03620532, 0.09208703, 0.0630683, 0.16039053, 0.16029632, 0.05789396, 0.09689599, 0.1643008, 0.11483827, 0.09845558, 0.109515995, 0.09842533]\n"
     ]
    }
   ],
   "source": [
    "print(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.37895499\n",
      "Iteration 2, loss = 0.21935842\n",
      "Iteration 3, loss = 0.21237948\n",
      "Iteration 4, loss = 0.20388629\n",
      "Iteration 5, loss = 0.20679686\n",
      "Iteration 6, loss = 0.19861788\n",
      "Iteration 7, loss = 0.19251979\n",
      "Iteration 8, loss = 0.18577758\n",
      "Iteration 9, loss = 0.18734341\n",
      "Iteration 10, loss = 0.18314035\n",
      "Iteration 11, loss = 0.24255371\n",
      "Iteration 12, loss = 0.18793118\n",
      "Iteration 13, loss = 0.18473389\n",
      "Iteration 14, loss = 0.18044678\n",
      "Iteration 15, loss = 0.17987966\n",
      "Iteration 16, loss = 0.18516261\n",
      "Iteration 17, loss = 0.19398217\n",
      "Iteration 18, loss = 0.17686959\n",
      "Iteration 19, loss = 0.17729695\n",
      "Iteration 20, loss = 0.18568534\n",
      "Iteration 21, loss = 0.18118245\n",
      "Iteration 22, loss = 0.17870537\n",
      "Iteration 23, loss = 0.17436809\n",
      "Iteration 24, loss = 0.18400726\n",
      "Iteration 25, loss = 0.17788367\n",
      "Iteration 26, loss = 0.18032708\n",
      "Iteration 27, loss = 0.17964557\n",
      "Iteration 28, loss = 0.16753421\n",
      "Iteration 29, loss = 0.17045854\n",
      "Iteration 30, loss = 0.18782817\n",
      "Iteration 31, loss = 0.17504088\n",
      "Iteration 32, loss = 0.17008026\n",
      "Iteration 33, loss = 0.18049117\n",
      "Iteration 34, loss = 0.19798060\n",
      "Iteration 35, loss = 0.17048162\n",
      "Iteration 36, loss = 0.17657747\n",
      "Iteration 37, loss = 0.18166561\n",
      "Iteration 38, loss = 0.18554881\n",
      "Iteration 39, loss = 0.17313410\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "mlp = MLPClassifier(max_iter=500)\n",
    "parameter_space = {\n",
    "    'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,), (250,), (500,) ],\n",
    "    'activation': ['tanh', 'relu', 'sigmoid'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'alpha': [0.0001, 0.05, .005, .0005],\n",
    "    'learning_rate': ['constant','adaptive'],\n",
    "    'learning_rate_init': [0.01, .001, .005, .0001, .0005, .00001],\n",
    "    'verbose': [True]\n",
    "}\n",
    "\n",
    "clf = GridSearchCV(mlp, parameter_space, n_jobs=-1, cv=3)\n",
    "clf.fit(overallDataX, kerasYW2V)\n",
    "y_pred = clf.predict(overallDataXDev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9476390836839644\n",
      "0.08547008547008547\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(kerasYDevW2VG, predictions)\n",
    "# precision_score = precision_score(kerasYDevW2VG, predictions)\n",
    "recall = recall_score(kerasYDevW2VG, predictions)\n",
    "\n",
    "print(accuracy)\n",
    "# print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.neural_network import MLPClassifier\n",
    "# clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(100, 36, 1), random_state=1)\n",
    "# clf.fit(X, y)\n",
    "\n",
    "y_pred = clf.predict_proba(overallDataXDev)\n",
    "predictions = []\n",
    "for i in y_pred:\n",
    "    if i[0] > .9:\n",
    "        predictions.append(0)\n",
    "    else:\n",
    "        predictions.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8536699392239364\n",
      "0.24345549738219896\n",
      "0.7948717948717948\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(kerasYDevW2VG, predictions)\n",
    "precision = precision_score(kerasYDevW2VG, predictions)\n",
    "recall = recall_score(kerasYDevW2VG, predictions)\n",
    "\n",
    "print(accuracy)\n",
    "print(precision)\n",
    "print(recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isInt(s):\n",
    "    try: \n",
    "        int(s)\n",
    "        return True\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "cols1=['index', 'mrn', 'vector']\n",
    "\n",
    "notes3= pd.read_csv('merged-testNotesAbridged.csv', header=None, error_bad_lines=False, nrows=6412, encoding='latin-1')\n",
    "\n",
    "for index, row in notes3.iterrows():\n",
    "    if not isInt(row[1]):\n",
    "        notes3 = notes3.drop([index])\n",
    "\n",
    "notes3 = notes2.rename(columns={0: \"index\", 1: \"mrn\", 2: \"noteType\", 3: \"noteText\"})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "cols1=['index', 'mrn', 'noteType', 'noteText']\n",
    "cols2=['mrn', 'label']\n",
    "\n",
    "labels = pd.read_csv('test_data.csv', header=None, names=cols2)\n",
    "\n",
    "notes['mrn'].replace('', np.nan, inplace=True)\n",
    "notes.dropna(subset=['mrn'], inplace=True)\n",
    "\n",
    "notes['mrn'] = notes['mrn'].astype(int)\n",
    "labels['mrn'] = labels['mrn'].astype(int)\n",
    "\n",
    "notesAndLabels = pd.merge(notes, labels, how='left', on=['mrn'])\n",
    "\n",
    "kerasX = np.asarray(notesAndLabels['noteText'])\n",
    "kerasY = np.asarray(notesAndLabels['label'])\n",
    "print(len(kerasY))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = vectorizer.fit_transform(kerasX)\n",
    "Xpred = vectorizer.transform(kerasXDev)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "FeaturizingMedicalNotes.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
